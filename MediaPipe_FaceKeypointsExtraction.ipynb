{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c54d8ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Capture Path: data/Video_chunks/Video_chunks\\trial_lie_001_000.mp4\n",
      "Saved with shape:\n",
      "class_label_npy.shape: (1,)\n",
      "sample_coordinates_data.shape: (126, 3)\n",
      "sample_audio.shape: (185661, 2)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import traceback\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# for audio / video extraction, conversion:\n",
    "import moviepy.editor as moviepyEditor\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "def extract_audio_from_video(video_path):  # , save_mp3_audio_as):\n",
    "    \n",
    "    # define clip from path:\n",
    "    clip = moviepyEditor.VideoFileClip(video_path)\n",
    "    \n",
    "    # extract audio:\n",
    "    return clip.audio.to_soundarray()\n",
    "    # clip.audio.write_audiofile(save_mp3_audio_as)\n",
    "    \n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "# run through single video:\n",
    "def process_video(video_path, class_name, webcam=False, verbose=False):\n",
    "    \n",
    "    detected_keypoints_coordinates = []\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # Media Pipe Initialization:\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    mp_drawing_styles = mp.solutions.drawing_styles\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    print(\"Video Capture Path:\", video_path)\n",
    "    \n",
    "    # For webcam input:\n",
    "    drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        max_num_faces=1,\n",
    "        refine_landmarks=True,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5) as face_mesh:\n",
    "        while cap.isOpened():\n",
    "            success, image = cap.read()\n",
    "            if not success:\n",
    "                if webcam:\n",
    "                    print(\"Ignoring empty camera frame.\")\n",
    "                    # If loading a video, use 'break' instead of 'continue'.\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "            # To improve performance, optionally mark the image as not writeable to\n",
    "            # pass by reference.\n",
    "            image.flags.writeable = False\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            results = face_mesh.process(image)\n",
    "\n",
    "            # Draw the face mesh annotations on the image.\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            '''\n",
    "            Collection of detected/tracked faces, where each face is represented as a list of 468 face landmarks and \n",
    "            each landmark is composed of x, y and z. x and y are normalized to [0.0, 1.0] by the image width and height \n",
    "            respectively. z represents the landmark depth with the depth at center of the head being the origin, \n",
    "            and the smaller the value the closer the landmark is to the camera. The magnitude of z uses roughly the same\n",
    "            scale as x.\n",
    "            '''\n",
    "            if results.multi_face_landmarks:\n",
    "                for face in results.multi_face_landmarks:\n",
    "                    for landmark in face.landmark:\n",
    "                        x = landmark.x\n",
    "                        y = landmark.y\n",
    "                        z = landmark.z\n",
    "\n",
    "                        if verbose:\n",
    "                            shape = image.shape \n",
    "                            relative_x = int(x * shape[1])\n",
    "                            relative_y = int(y * shape[0])\n",
    "\n",
    "                            cv2.circle(image, (relative_x, relative_y), radius=1, color=(225, 0, 100), thickness=1)\n",
    "            else:\n",
    "                # print(traceback.format_exc())\n",
    "                x, y, z = 0.0, 0.0, 0.0\n",
    "                \n",
    "            \n",
    "            # Append detected_keypoints_coordinates:\n",
    "            detected_keypoints_coordinates.append([x,y,z])                \n",
    "            \n",
    "            if verbose:\n",
    "                cv2.imshow('MediaPipe Face Mesh', cv2.flip(image, 1))\n",
    "            \n",
    "            if cv2.waitKey(20) & 0xFF == 27:\n",
    "                break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    return np.array(detected_keypoints_coordinates)\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "def mkdir_if_none(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------    \n",
    "def save_label_data(class_num, sample_coordinates_data, sample_audio, save_as):\n",
    "    \n",
    "    class_label_npy = np.array([class_num])\n",
    "    # sample_coordinates_data\n",
    "    \n",
    "    save_label_as = f\"{save_as}_label.npy\"\n",
    "    save_coordinates_as = f\"{save_as}_coord.npy\"\n",
    "    save_audio_as = f\"{save_as}_audio.npy\"\n",
    "    \n",
    "    np.save(save_label_as, class_label_npy)\n",
    "    np.save(save_coordinates_as, sample_coordinates_data)\n",
    "    np.save(save_audio_as, sample_audio)\n",
    "    \n",
    "    print(\"Saved with shape:\")\n",
    "    print(\"class_label_npy.shape:\", class_label_npy.shape)\n",
    "    print(\"sample_coordinates_data.shape:\", sample_coordinates_data.shape)\n",
    "    print(\"sample_audio.shape:\", sample_audio.shape)\n",
    "    \n",
    "\n",
    "#------------------------------------------------------------------------------------\n",
    "# run through all videos and collect - (1) face features and (2) audio features, and (3) save label reference if needed:\n",
    "def train_data_preparation(video_data_dir=\"data/Video_chunks/Video_chunks\",\n",
    "                           csv_path=\"data/Video_chunks/Labels.xlsx\",\n",
    "                           class_to_num = {\"truth\": 0, \"lie\": 1},\n",
    "                           num_to_class = {0: \"truth\", 1: \"lie\"},\n",
    "                           save_keypoints_npy_to=os.path.join(os.getcwd(), \"mediaPipe_keypoints_data\")):\n",
    "    \n",
    "    mkdir_if_none(save_keypoints_npy_to)\n",
    "    \n",
    "    # read train data:\n",
    "    csv_data = pd.read_excel(csv_path, sheet_name=\"All_Gestures_Deceptive and Trut\")\n",
    "    \n",
    "    # go over each video and collect face features + audio features:\n",
    "    for video_name in os.listdir(video_data_dir):\n",
    "        # collect information about video:\n",
    "        class_name = video_name.split(\"_\")[1]\n",
    "        class_num = class_to_num[class_name]\n",
    "        video_path = os.path.join(video_data_dir, video_name)\n",
    "        sample_name = video_name.split(\".\")[0]\n",
    "        save_as = os.path.join(save_keypoints_npy_to, sample_name)\n",
    "        \n",
    "        # run through frames of current video:\n",
    "        extracted_keypoints_npy = process_video(video_path=video_path, class_name=class_name, verbose=True)\n",
    "        \n",
    "        # extract audio:\n",
    "        extracted_audio_npy = extract_audio_from_video(video_path=video_path)  # , save_mp3_audio_as=\"sample_audio.mp3\")\n",
    "        \n",
    "        # save training sample:\n",
    "        save_label_data(class_num=class_num, \n",
    "                        sample_coordinates_data=extracted_keypoints_npy, sample_audio=extracted_audio_npy, \n",
    "                        save_as=save_as)\n",
    "        \n",
    "        break\n",
    "\n",
    "\n",
    "# run video processing for (face & audio) feature collection:\n",
    "train_data_preparation()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c876ecc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
