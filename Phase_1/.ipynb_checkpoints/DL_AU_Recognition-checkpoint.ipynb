{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80b08d3e",
   "metadata": {},
   "source": [
    "# DeepFace Review:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd10f51d",
   "metadata": {},
   "source": [
    "### These models DeepFace uses in the background are:\n",
    "* VGG-Face\n",
    "* Google FaceNet\n",
    "* OpenFace\n",
    "* Facebook DeepFace\n",
    "* DeepID\n",
    "* ArcFace\n",
    "* Dlib\n",
    "\n",
    "### These models are so good that they have demonstrated that they can analyze images of faces (and even videos) at a level that surpasses what is humanly possible. The face recognition pipeline of DeepFace consists of four stages:\n",
    "* Detection \n",
    "* Alignment\n",
    "* Representation \n",
    "* Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1222f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting deepface\n",
      "  Downloading deepface-0.0.79-py3-none-any.whl (49 kB)\n",
      "     ---------------------------------------- 49.6/49.6 kB 2.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas>=0.23.4 in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from deepface) (1.5.3)\n",
      "Collecting tensorflow>=1.9.0\n",
      "  Downloading tensorflow-2.11.0-cp310-cp310-win_amd64.whl (1.9 kB)\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from deepface) (1.24.2)\n",
      "Requirement already satisfied: tqdm>=4.30.0 in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from deepface) (4.64.1)\n",
      "Requirement already satisfied: opencv-python>=4.5.5.64 in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from deepface) (4.7.0.72)\n",
      "Collecting keras>=2.2.0\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 1.1 MB/s eta 0:00:00\n",
      "Collecting retina-face>=0.0.1\n",
      "  Downloading retina_face-0.0.13-py3-none-any.whl (16 kB)\n",
      "Collecting mtcnn>=0.1.0\n",
      "  Downloading mtcnn-0.1.1-py3-none-any.whl (2.3 MB)\n",
      "     ---------------------------------------- 2.3/2.3 MB 935.2 kB/s eta 0:00:00\n",
      "Requirement already satisfied: Pillow>=5.2.0 in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from deepface) (9.4.0)\n",
      "Collecting fire>=0.4.0\n",
      "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
      "     ---------------------------------------- 88.3/88.3 kB 1.3 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting Flask>=1.1.2\n",
      "  Downloading Flask-2.2.3-py3-none-any.whl (101 kB)\n",
      "     -------------------------------------- 101.8/101.8 kB 1.9 MB/s eta 0:00:00\n",
      "Collecting gunicorn>=20.1.0\n",
      "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
      "     ---------------------------------------- 79.5/79.5 kB 2.2 MB/s eta 0:00:00\n",
      "Collecting gdown>=3.10.1\n",
      "  Downloading gdown-4.6.4-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: six in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from fire>=0.4.0->deepface) (1.16.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
      "Requirement already satisfied: Jinja2>=3.0 in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from Flask>=1.1.2->deepface) (3.1.2)\n",
      "Collecting click>=8.0\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "     ---------------------------------------- 96.6/96.6 kB 1.4 MB/s eta 0:00:00\n",
      "Collecting itsdangerous>=2.0\n",
      "  Downloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
      "Collecting Werkzeug>=2.2.2\n",
      "  Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
      "     -------------------------------------- 233.6/233.6 kB 1.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from gdown>=3.10.1->deepface) (2.28.2)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.9.0-py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from gdown>=3.10.1->deepface) (4.11.2)\n",
      "Requirement already satisfied: setuptools>=3.0 in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from gunicorn>=20.1.0->deepface) (65.6.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from pandas>=0.23.4->deepface) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from pandas>=0.23.4->deepface) (2.8.2)\n",
      "Collecting tensorflow-intel==2.11.0\n",
      "  Downloading tensorflow_intel-2.11.0-cp310-cp310-win_amd64.whl (266.3 MB)\n",
      "     ------------------------------------ 266.3/266.3 MB 928.1 kB/s eta 0:00:00\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow>=1.9.0->deepface) (23.1.21)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     ---------------------------------------- 57.5/57.5 kB 1.0 MB/s eta 0:00:00\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 891.3 kB/s eta 0:00:00\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     -------------------------------------- 65.5/65.5 kB 891.1 kB/s eta 0:00:00\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp310-cp310-win_amd64.whl (895 kB)\n",
      "     ------------------------------------ 895.7/895.7 kB 776.7 kB/s eta 0:00:00\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.51.3-cp310-cp310-win_amd64.whl (3.7 MB)\n",
      "     ---------------------------------------- 3.7/3.7 MB 1.1 MB/s eta 0:00:00\n",
      "Collecting tensorboard<2.12,>=2.11\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "     ---------------------------------------- 6.0/6.0 MB 943.0 kB/s eta 0:00:00\n",
      "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "     -------------------------------------- 439.2/439.2 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.15.0-cp310-cp310-win_amd64.whl (36 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\lrspr\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.11.0->tensorflow>=1.9.0->deepface) (1.3.0)\n",
      "Collecting typing-extensions>=3.6.6\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.8.0-cp310-cp310-win_amd64.whl (2.6 MB)\n",
      "     ---------------------------------------- 2.6/2.6 MB 838.2 kB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow>=1.9.0->deepface) (23.0)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-15.0.6.1-py2.py3-none-win_amd64.whl (23.2 MB)\n",
      "     -------------------------------------- 23.2/23.2 MB 882.2 kB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from tqdm>=4.30.0->deepface) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from Jinja2>=3.0->Flask>=1.1.2->deepface) (2.1.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from beautifulsoup4->gdown>=3.10.1->deepface) (2.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from requests[socks]->gdown>=3.10.1->deepface) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from requests[socks]->gdown>=3.10.1->deepface) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from requests[socks]->gdown>=3.10.1->deepface) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from requests[socks]->gdown>=3.10.1->deepface) (3.4)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow>=1.9.0->deepface) (0.38.4)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "     -------------------------------------- 781.3/781.3 kB 1.0 MB/s eta 0:00:00\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "     ---------------------------------------- 93.3/93.3 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.16.2-py2.py3-none-any.whl (177 kB)\n",
      "     -------------------------------------- 177.2/177.2 kB 1.5 MB/s eta 0:00:00\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "     -------------------------------------- 155.3/155.3 kB 1.0 MB/s eta 0:00:00\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "     -------------------------------------- 77.1/77.1 kB 846.9 kB/s eta 0:00:00\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     ------------------------------------ 151.7/151.7 kB 645.9 kB/s eta 0:00:00\n",
      "Building wheels for collected packages: fire\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116947 sha256=d2e5328a6ffaae8e6a1e3085fe431534112cb4ae776650b9ccda31d5029a6b04\n",
      "  Stored in directory: C:\\Users\\lrspr\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-kiyz_7b8\\wheels\\c4\\eb\\6a\\1c6d2ad660043768e998bdf9c6a28db2f1b7db3a5825d51e87\n",
      "Successfully built fire\n",
      "Installing collected packages: tensorboard-plugin-wit, pyasn1, libclang, wrapt, Werkzeug, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, PySocks, pyasn1-modules, protobuf, opt-einsum, oauthlib, markdown, keras, itsdangerous, h5py, gunicorn, grpcio, google-pasta, gast, filelock, click, cachetools, astunparse, requests-oauthlib, mtcnn, google-auth, Flask, fire, google-auth-oauthlib, gdown, tensorboard, tensorflow-intel, tensorflow, retina-face, deepface\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\lrspr\\\\anaconda3\\\\envs\\\\caut_env\\\\Lib\\\\site-packages\\\\google\\\\~rotobuf\\\\internal\\\\_api_implementation.cp310-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install deepface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2cec606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e900d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "race_model_single_batch.h5 will be downloaded...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://github.com/serengil/deepface_models/releases/download/v1.0/race_model_single_batch.h5\n",
      "To: C:\\Users\\lrspr\\.deepface\\weights\\race_model_single_batch.h5\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 537M/537M [00:14<00:00, 37.7MB/s]\n",
      "Action: race: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face_analysis for frame {frame_count}: [{'emotion': {'angry': 0.16445438377559185, 'disgust': 0.020809698617085814, 'fear': 48.508599400520325, 'happy': 0.00018318594356969697, 'sad': 51.29292011260986, 'surprise': 9.269933798350394e-06, 'neutral': 0.013024600048083812}, 'dominant_emotion': 'sad', 'region': {'x': 6, 'y': 10, 'w': 117, 'h': 117}, 'age': 22, 'gender': {'Woman': 0.019238154345657676, 'Man': 99.98076558113098}, 'dominant_gender': 'Man', 'race': {'asian': 4.685890302062035, 'indian': 15.215986967086792, 'black': 1.988225057721138, 'white': 19.47016716003418, 'middle eastern': 38.63554298877716, 'latino hispanic': 20.004186034202576}, 'dominant_race': 'middle eastern'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  6.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face_analysis for frame {frame_count}: [{'emotion': {'angry': 63.53838282366004, 'disgust': 6.986252165504953e-06, 'fear': 28.710975800636316, 'happy': 0.08826407832298239, 'sad': 6.554166191319681, 'surprise': 0.00021591171998808844, 'neutral': 1.1079857667701523}, 'dominant_emotion': 'angry', 'region': {'x': 28, 'y': 18, 'w': 82, 'h': 82}, 'age': 22, 'gender': {'Woman': 0.8673837408423424, 'Man': 99.13261532783508}, 'dominant_gender': 'Man', 'race': {'asian': 24.71116034455063, 'indian': 3.4909630412437345, 'black': 0.7215829064895455, 'white': 38.831653525331824, 'middle eastern': 10.651949682947974, 'latino hispanic': 21.592695807975282}, 'dominant_race': 'white'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face_analysis for frame {frame_count}: [{'emotion': {'angry': 0.5664533004164696, 'disgust': 0.00042275200939911883, 'fear': 0.9944024495780468, 'happy': 95.0090229511261, 'sad': 1.5692507848143578, 'surprise': 6.648767092443109e-08, 'neutral': 1.860448345541954}, 'dominant_emotion': 'happy', 'region': {'x': 16, 'y': 9, 'w': 108, 'h': 108}, 'age': 24, 'gender': {'Woman': 0.021049194037914276, 'Man': 99.97895359992981}, 'dominant_gender': 'Man', 'race': {'asian': 5.457611382007599, 'indian': 3.8705047219991684, 'black': 0.3328523598611355, 'white': 47.75943160057068, 'middle eastern': 27.54383385181427, 'latino hispanic': 15.035766363143921}, 'dominant_race': 'white'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  6.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face_analysis for frame {frame_count}: [{'emotion': {'angry': 0.003238333203832922, 'disgust': 7.425792193004524e-05, 'fear': 0.40085783754931914, 'happy': 6.487934843056153e-06, 'sad': 99.59446189370524, 'surprise': 9.496948238778031e-08, 'neutral': 0.0013610525358653276}, 'dominant_emotion': 'sad', 'region': {'x': 7, 'y': 14, 'w': 112, 'h': 112}, 'age': 24, 'gender': {'Woman': 0.027011553174816072, 'Man': 99.97298121452332}, 'dominant_gender': 'Man', 'race': {'asian': 5.094516277313232, 'indian': 11.26137599349022, 'black': 0.966689083725214, 'white': 24.360056221485138, 'middle eastern': 39.53970670700073, 'latino hispanic': 18.7776580452919}, 'dominant_race': 'middle eastern'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face_analysis for frame {frame_count}: [{'emotion': {'angry': 0.4516325963744636, 'disgust': 7.786516467235094e-05, 'fear': 0.0928959978191999, 'happy': 68.25441339664619, 'sad': 0.46200203628862335, 'surprise': 0.00016273221156519542, 'neutral': 30.738816349365276}, 'dominant_emotion': 'happy', 'region': {'x': 13, 'y': 8, 'w': 106, 'h': 106}, 'age': 23, 'gender': {'Woman': 0.023082912957761437, 'Man': 99.97691512107849}, 'dominant_gender': 'Man', 'race': {'asian': 1.1109860220866177, 'indian': 1.071441045827741, 'black': 0.04227985706010675, 'white': 68.41239113659964, 'middle eastern': 23.063574962189797, 'latino hispanic': 6.2993228376718955}, 'dominant_race': 'white'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face_analysis for frame {frame_count}: [{'emotion': {'angry': 5.502072304610747, 'disgust': 3.280994089912591e-05, 'fear': 4.4230121030218665, 'happy': 88.49498560994778, 'sad': 1.5646570363436363, 'surprise': 6.503586390714394e-08, 'neutral': 0.015243386768807427}, 'dominant_emotion': 'happy', 'region': {'x': 9, 'y': 13, 'w': 106, 'h': 106}, 'age': 23, 'gender': {'Woman': 0.025238297530449927, 'Man': 99.97475743293762}, 'dominant_gender': 'Man', 'race': {'asian': 0.4686873219869925, 'indian': 0.9202726733847065, 'black': 0.029513640342161973, 'white': 71.16716332239886, 'middle eastern': 21.714481515555473, 'latino hispanic': 5.699886348836801}, 'dominant_race': 'white'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  6.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face_analysis for frame {frame_count}: [{'emotion': {'angry': 0.0011990005597503127, 'disgust': 2.0278186659797956e-11, 'fear': 58.83839831809978, 'happy': 0.809779766622754, 'sad': 31.966436739735215, 'surprise': 1.8944446049822712e-07, 'neutral': 8.38419373443052}, 'dominant_emotion': 'fear', 'region': {'x': 10, 'y': 12, 'w': 111, 'h': 111}, 'age': 24, 'gender': {'Woman': 0.01526308769825846, 'Man': 99.98472929000854}, 'dominant_gender': 'Man', 'race': {'asian': 0.7022295612841845, 'indian': 1.5377165749669075, 'black': 0.02637903962749988, 'white': 63.223785161972046, 'middle eastern': 29.591837525367737, 'latino hispanic': 4.91805300116539}, 'dominant_race': 'white'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face_analysis for frame {frame_count}: [{'emotion': {'angry': 0.010752374423343654, 'disgust': 4.240467171327364e-10, 'fear': 30.44931772049175, 'happy': 0.2761572531182917, 'sad': 68.33419611061788, 'surprise': 2.6155001946677164e-08, 'neutral': 0.9295776109145648}, 'dominant_emotion': 'sad', 'region': {'x': 11, 'y': 9, 'w': 110, 'h': 110}, 'age': 24, 'gender': {'Woman': 0.011281771003268659, 'Man': 99.9887228012085}, 'dominant_gender': 'Man', 'race': {'asian': 3.2010240525617633, 'indian': 1.7239219605280138, 'black': 0.05747891606637335, 'white': 62.94852274349467, 'middle eastern': 27.08708209318412, 'latino hispanic': 4.981972574113165}, 'dominant_race': 'white'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face_analysis for frame {frame_count}: [{'emotion': {'angry': 0.019039519247598946, 'disgust': 9.448916800901264e-12, 'fear': 70.62098979949951, 'happy': 11.408849060535431, 'sad': 17.42592304944992, 'surprise': 2.9850530380848284e-08, 'neutral': 0.5252011585980654}, 'dominant_emotion': 'fear', 'region': {'x': 8, 'y': 8, 'w': 111, 'h': 111}, 'age': 24, 'gender': {'Woman': 0.028737500542774796, 'Man': 99.97126460075378}, 'dominant_gender': 'Man', 'race': {'asian': 1.2427739608451105, 'indian': 0.7147054613951124, 'black': 0.021811974720066768, 'white': 76.39000871121935, 'middle eastern': 17.91949229952647, 'latino hispanic': 3.711205569577549}, 'dominant_race': 'white'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  6.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face_analysis for frame {frame_count}: [{'emotion': {'angry': 0.07345395279116929, 'disgust': 6.801212371065901e-09, 'fear': 33.289867639541626, 'happy': 2.7176694944500923, 'sad': 29.712364077568054, 'surprise': 2.190762948828251e-06, 'neutral': 34.20664668083191}, 'dominant_emotion': 'neutral', 'region': {'x': 16, 'y': 7, 'w': 106, 'h': 106}, 'age': 24, 'gender': {'Woman': 0.006502530595753342, 'Man': 99.99349117279053}, 'dominant_gender': 'Man', 'race': {'asian': 0.8710547350347042, 'indian': 4.090525209903717, 'black': 0.11234348639845848, 'white': 36.22295260429382, 'middle eastern': 49.76094663143158, 'latino hispanic': 8.942172676324844}, 'dominant_race': 'middle eastern'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face_analysis for frame {frame_count}: [{'emotion': {'angry': 0.00023331012042964363, 'disgust': 5.744577465421513e-10, 'fear': 2.786562385354184, 'happy': 3.6740625604428385, 'sad': 63.21024079383855, 'surprise': 1.682710848815828e-07, 'neutral': 30.328901429706896}, 'dominant_emotion': 'sad', 'region': {'x': 15, 'y': 8, 'w': 106, 'h': 106}, 'age': 24, 'gender': {'Woman': 0.019204798445571214, 'Man': 99.98080134391785}, 'dominant_gender': 'Man', 'race': {'asian': 6.986675411462784, 'indian': 5.815142393112183, 'black': 0.20114597864449024, 'white': 38.47131431102753, 'middle eastern': 39.35358226299286, 'latino hispanic': 9.172140061855316}, 'dominant_race': 'middle eastern'}]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "from deepface import DeepFace\n",
    "    \n",
    "cap = cv2.VideoCapture(0)\n",
    "pTime = 0\n",
    " \n",
    "mpFaceDetection = mp.solutions.face_detection\n",
    "mpDraw = mp.solutions.drawing_utils\n",
    "faceDetection = mpFaceDetection.FaceDetection(0.75)\n",
    "\n",
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    \n",
    "    try:\n",
    " \n",
    "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        results = faceDetection.process(imgRGB)\n",
    "\n",
    "        if results.detections:\n",
    "            for id, detection in enumerate(results.detections):\n",
    "                # mpDraw.draw_detection(img, detection)\n",
    "                # print(id, detection)\n",
    "                # print(detection.score)\n",
    "                # print(detection.location_data.relative_bounding_box)\n",
    "                bboxC = detection.location_data.relative_bounding_box\n",
    "                ih, iw, ic = img.shape\n",
    "                bbox = int(bboxC.xmin * iw), int(bboxC.ymin * ih), \\\n",
    "                       int(bboxC.width * iw), int(bboxC.height * ih)\n",
    "                y_start, y_end = bbox[1], (bbox[1]+bbox[3])\n",
    "                x_start, x_end = bbox[0], (bbox[0]+bbox[2])\n",
    "                face_cropped = img[y_start:y_end, x_start:x_end]\n",
    "                cv2.rectangle(img, bbox, (255, 0, 255), 2)\n",
    "                face_analysis = DeepFace.analyze(face_cropped)\n",
    "                print(\"face_analysis for frame {frame_count}:\", face_analysis)\n",
    "                frame_count+=1\n",
    "                # cv2.putText(img, f'{int(detection.score[0] * 100)}%',\n",
    "                #             (bbox[0], bbox[1] - 20), cv2.FONT_HERSHEY_PLAIN,\n",
    "                #             2, (255, 0, 255), 2)\n",
    "\n",
    "        cTime = time.time()\n",
    "        fps = 1 / (cTime - pTime)\n",
    "        pTime = cTime\n",
    "        # cv2.putText(img, f'FPS: {int(fps)}', (20, 70), cv2.FONT_HERSHEY_PLAIN,\n",
    "        #             3, (0, 255, 0), 2)\n",
    "        cv2.imshow(\"Image\", face_cropped)\n",
    "        cv2.waitKey(1)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a47400",
   "metadata": {},
   "source": [
    "# DeepFace insights:\n",
    "### Does not provide Action Units, but only provides emotions instead. This is not sufficient for our use. Furthermore, the FPS when running with DeepFace is very low."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02a3cc6",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23ff904",
   "metadata": {},
   "source": [
    "# Face Alignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba0ad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "\n",
    "from face_geometry import get_metric_landmarks, PCF, canonical_metric_landmarks, procrustes_landmark_basis\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "points_idx = [33,263,61,291,199]\n",
    "points_idx = points_idx + [key for (key,val) in procrustes_landmark_basis]\n",
    "points_idx = list(set(points_idx))\n",
    "points_idx.sort()\n",
    "# points_idx = list(range(0,468)); points_idx[0:2] = points_idx[0:2:-1];\n",
    "\n",
    "frame_height, frame_width, channels = (720, 1280, 3)\n",
    "\n",
    "# pseudo camera internals\n",
    "focal_length = frame_width\n",
    "center = (frame_width/2, frame_height/2)\n",
    "camera_matrix = np.array(\n",
    "                         [[focal_length, 0, center[0]],\n",
    "                         [0, focal_length, center[1]],\n",
    "                         [0, 0, 1]], dtype = \"double\"\n",
    "                         )\n",
    "\n",
    "dist_coeff = np.zeros((4, 1))\n",
    "\n",
    "def main():\n",
    "    source = WebcamSource()\n",
    "\n",
    "    pcf = PCF(near=1,far=10000,frame_height=frame_height,frame_width=frame_width,fy=camera_matrix[1,1])\n",
    "\n",
    "    for idx, (frame, frame_rgb) in enumerate(source):\n",
    "\n",
    "        # print(idx)\n",
    "        \n",
    "        face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "        results = face_mesh.process(frame)\n",
    "        multi_face_landmarks = results.multi_face_landmarks\n",
    "\n",
    "        if multi_face_landmarks:\n",
    "            face_landmarks = results.multi_face_landmarks[0]\n",
    "            landmarks = np.array([(lm.x,lm.y,lm.z) for lm in face_landmarks.landmark])\n",
    "            landmarks = landmarks.T\n",
    "\n",
    "            metric_landmarks, pose_transform_mat = get_metric_landmarks(landmarks.copy(), pcf)\n",
    "            model_points = metric_landmarks[0:3, points_idx].T\n",
    "            image_points = landmarks[0:2, points_idx].T * np.array([frame_width, frame_height])[None,:]\n",
    "\n",
    "            success, rotation_vector, translation_vector = cv2.solvePnP(model_points, image_points, camera_matrix, dist_coeff, flags=cv2.cv2.SOLVEPNP_ITERATIVE)\n",
    "            # _, rotation_vector, translation_vector, inliers = cv2.solvePnPRansac(model_points, image_points, camera_matrix, dist_coeff)\n",
    "\n",
    "            (nose_end_point2D, jacobian) = cv2.projectPoints(np.array([(0.0, 0.0, 25.0)]), rotation_vector, translation_vector, camera_matrix, dist_coeff)\n",
    "\n",
    "            for ii in points_idx: # range(landmarks.shape[1]):\n",
    "                pos = np.array((frame_width*landmarks[0, ii], frame_height*landmarks[1, ii])).astype(np.int32)\n",
    "                frame = cv2.circle(frame, tuple(pos), 1, (0, 255, 255), -1)\n",
    "\n",
    "            p1 = ( int(image_points[0][0]), int(image_points[0][1]))\n",
    "            p2 = ( int(nose_end_point2D[0][0][0]), int(nose_end_point2D[0][0][1]))\n",
    "\n",
    "            frame = cv2.line(frame, p1, p2, (255,0,0), 2)\n",
    "\n",
    "\n",
    "\n",
    "        source.show(frame)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929a04d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from webcamsource import WebcamSource\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "\n",
    "from face_geometry import get_metric_landmarks, PCF, canonical_metric_landmarks, procrustes_landmark_basis\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "points_idx = [33,263,61,291,199]\n",
    "points_idx = points_idx + [key for (key,val) in procrustes_landmark_basis]\n",
    "points_idx = list(set(points_idx))\n",
    "points_idx.sort()\n",
    "# points_idx = list(range(0,468)); points_idx[0:2] = points_idx[0:2:-1];\n",
    "\n",
    "frame_height, frame_width, channels = (720, 1280, 3)\n",
    "\n",
    "# pseudo camera internals\n",
    "focal_length = frame_width\n",
    "center = (frame_width/2, frame_height/2)\n",
    "camera_matrix = np.array(\n",
    "                         [[focal_length, 0, center[0]],\n",
    "                         [0, focal_length, center[1]],\n",
    "                         [0, 0, 1]], dtype = \"double\"\n",
    "                         )\n",
    "\n",
    "dist_coeff = np.zeros((4, 1))\n",
    "\n",
    "def main():\n",
    "    source = WebcamSource()\n",
    "\n",
    "    pcf = PCF(near=1,far=10000,frame_height=frame_height,frame_width=frame_width,fy=camera_matrix[1,1])\n",
    "\n",
    "    for idx, (frame, frame_rgb) in enumerate(source):\n",
    "\n",
    "        # print(idx)\n",
    "        \n",
    "        face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "        results = face_mesh.process(frame)\n",
    "        multi_face_landmarks = results.multi_face_landmarks\n",
    "\n",
    "        if multi_face_landmarks:\n",
    "            face_landmarks = results.multi_face_landmarks[0]\n",
    "            landmarks = np.array([(lm.x,lm.y,lm.z) for lm in face_landmarks.landmark])\n",
    "            landmarks = landmarks.T\n",
    "\n",
    "            metric_landmarks, pose_transform_mat = get_metric_landmarks(landmarks.copy(), pcf)\n",
    "            model_points = metric_landmarks[0:3, points_idx].T\n",
    "            image_points = landmarks[0:2, points_idx].T * np.array([frame_width, frame_height])[None,:]\n",
    "\n",
    "            success, rotation_vector, translation_vector = cv2.solvePnP(model_points, image_points, camera_matrix, dist_coeff, flags=cv2.cv2.SOLVEPNP_ITERATIVE)\n",
    "            # _, rotation_vector, translation_vector, inliers = cv2.solvePnPRansac(model_points, image_points, camera_matrix, dist_coeff)\n",
    "\n",
    "            (nose_end_point2D, jacobian) = cv2.projectPoints(np.array([(0.0, 0.0, 25.0)]), rotation_vector, translation_vector, camera_matrix, dist_coeff)\n",
    "\n",
    "            for ii in points_idx: # range(landmarks.shape[1]):\n",
    "                pos = np.array((frame_width*landmarks[0, ii], frame_height*landmarks[1, ii])).astype(np.int32)\n",
    "                frame = cv2.circle(frame, tuple(pos), 1, (0, 255, 255), -1)\n",
    "\n",
    "            p1 = ( int(image_points[0][0]), int(image_points[0][1]))\n",
    "            p2 = ( int(nose_end_point2D[0][0][0]), int(nose_end_point2D[0][0][1]))\n",
    "\n",
    "            frame = cv2.line(frame, p1, p2, (255,0,0), 2)\n",
    "\n",
    "\n",
    "\n",
    "        source.show(frame)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8a57409",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'webcamsource'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwebcamsource\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WebcamSource\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'webcamsource'"
     ]
    }
   ],
   "source": [
    "from webcamsource import WebcamSource\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "\n",
    "from face_geometry import get_metric_landmarks, PCF, canonical_metric_landmarks, procrustes_landmark_basis\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "points_idx = [33,263,61,291,199]\n",
    "points_idx = points_idx + [key for (key,val) in procrustes_landmark_basis]\n",
    "points_idx = list(set(points_idx))\n",
    "points_idx.sort()\n",
    "# points_idx = list(range(0,468)); points_idx[0:2] = points_idx[0:2:-1];\n",
    "\n",
    "frame_height, frame_width, channels = (720, 1280, 3)\n",
    "\n",
    "# pseudo camera internals\n",
    "focal_length = frame_width\n",
    "center = (frame_width/2, frame_height/2)\n",
    "camera_matrix = np.array(\n",
    "                         [[focal_length, 0, center[0]],\n",
    "                         [0, focal_length, center[1]],\n",
    "                         [0, 0, 1]], dtype = \"double\"\n",
    "                         )\n",
    "\n",
    "dist_coeff = np.zeros((4, 1))\n",
    "\n",
    "def main():\n",
    "    source = WebcamSource()\n",
    "\n",
    "    pcf = PCF(near=1,far=10000,frame_height=frame_height,frame_width=frame_width,fy=camera_matrix[1,1])\n",
    "\n",
    "    for idx, (frame, frame_rgb) in enumerate(source):\n",
    "\n",
    "        # print(idx)\n",
    "        \n",
    "        face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "        results = face_mesh.process(frame)\n",
    "        multi_face_landmarks = results.multi_face_landmarks\n",
    "\n",
    "        if multi_face_landmarks:\n",
    "            face_landmarks = results.multi_face_landmarks[0]\n",
    "            landmarks = np.array([(lm.x,lm.y,lm.z) for lm in face_landmarks.landmark])\n",
    "            landmarks = landmarks.T\n",
    "\n",
    "            metric_landmarks, pose_transform_mat = get_metric_landmarks(landmarks.copy(), pcf)\n",
    "            model_points = metric_landmarks[0:3, points_idx].T\n",
    "            image_points = landmarks[0:2, points_idx].T * np.array([frame_width, frame_height])[None,:]\n",
    "\n",
    "            success, rotation_vector, translation_vector = cv2.solvePnP(model_points, image_points, camera_matrix, dist_coeff, flags=cv2.cv2.SOLVEPNP_ITERATIVE)\n",
    "            # _, rotation_vector, translation_vector, inliers = cv2.solvePnPRansac(model_points, image_points, camera_matrix, dist_coeff)\n",
    "\n",
    "            (nose_end_point2D, jacobian) = cv2.projectPoints(np.array([(0.0, 0.0, 25.0)]), rotation_vector, translation_vector, camera_matrix, dist_coeff)\n",
    "\n",
    "            for ii in points_idx: # range(landmarks.shape[1]):\n",
    "                pos = np.array((frame_width*landmarks[0, ii], frame_height*landmarks[1, ii])).astype(np.int32)\n",
    "                frame = cv2.circle(frame, tuple(pos), 1, (0, 255, 255), -1)\n",
    "\n",
    "            p1 = ( int(image_points[0][0]), int(image_points[0][1]))\n",
    "            p2 = ( int(nose_end_point2D[0][0][0]), int(nose_end_point2D[0][0][1]))\n",
    "\n",
    "            frame = cv2.line(frame, p1, p2, (255,0,0), 2)\n",
    "\n",
    "\n",
    "\n",
    "        source.show(frame)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
