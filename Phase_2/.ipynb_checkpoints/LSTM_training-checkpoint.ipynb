{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2466add1",
   "metadata": {},
   "source": [
    "# Lie Recognition using LSTM + Dense Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d29e13",
   "metadata": {},
   "source": [
    "### Torch Dataset Creation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ee974de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import math\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode\n",
    "\n",
    "\n",
    "def load_coord_data(sample_data_path, frame_cap):\n",
    "    \n",
    "    if sample_data_path.endswith(\".npy\"):\n",
    "        load_sample = np.load(sample_data_path)[:frame_cap]\n",
    "    elif sample_data_path.endswith(\".csv\"):\n",
    "        load_sample = pd.read_csv(sample_data_path)[:frame_cap]\n",
    "        return load_sample\n",
    "        load_sample = load_sample.values()\n",
    "    else:\n",
    "        print(\">>> WARNING: No support for {sample_data_path}. Returning None.\")\n",
    "        load_sample = None\n",
    "    \n",
    "    return load_sample\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "class DeceptionDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, mode, collection_type, data_dir, csv_dir,\n",
    "                 class_to_num={\"truth\": 0, \"lie\": 1}, num_to_class={0: \"truth\", 1: \"lie\"}, transform=None,\n",
    "                 seconds_input_size=3, fps_min=29, keypoints_quantity=478, coordinate_amount=3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.data_df = pd.read_csv(os.path.join(csv_dir, f\"{mode}_DARE.csv\"))\n",
    "        print(\"frame_sum:\", sum(self.data_df[\"total_frame_count\"].tolist()))\n",
    "        self.transform = transform\n",
    "        if collection_type == \"MediaPipe\":\n",
    "            self.sample_postfix = \"_MP_coord.npy\"\n",
    "        elif collection_type == \"OpenFace\":\n",
    "            self.sample_postfix = \".csv\"\n",
    "        else:\n",
    "            print(\">>> WARNING: No such collection type was used before. Sample postfix append was set to blank.\")\n",
    "            self.sample_postfix = \"\"\n",
    "        self.class_to_num = class_to_num\n",
    "        self.num_to_class = num_to_class\n",
    "        self.frame_cap = fps_min * seconds_input_size\n",
    "        self.input_dim = (self.frame_cap*keypoints_quantity*coordinate_amount)  # torch.Size([4, 87, 478, 3])\n",
    "        self.keypoints_quantity = keypoints_quantity\n",
    "        self.coordinate_amount = coordinate_amount\n",
    "        # 126, 87 (29FPS*3=87), *45*\n",
    "        # 87\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        row_data = self.data_df.iloc[idx]\n",
    "        \n",
    "        # get path to data itself:\n",
    "        base_name = row_data[\"video_name\"].split(\".\")[0]\n",
    "        data_path = os.path.join(self.data_dir,\n",
    "                                 f\"{base_name}{self.sample_postfix}\").replace(\"\\\\\", \"/\")\n",
    "        \n",
    "        # get label:\n",
    "        label = self.class_to_num[row_data[\"label\"]]        \n",
    "        data  = load_coord_data(data_path, self.frame_cap)\n",
    "        \n",
    "        # if FPS is lower, duplicate every frame to increase (generate slow video as workaround):\n",
    "        [1,2,3] => desired size is 6/4 => 2 => [1,1,2,2,3,3]\n",
    "        \n",
    "        if data.shape[0] < self.frame_cap:\n",
    "            repeat_for = int(math.ceil(self.frame_cap / data.shape[0]))\n",
    "            data = np.repeat(data, repeat_for, axis=0)[:self.frame_cap]\n",
    "            # add additional edge case carry out:\n",
    "            if len(data) < self.frame_cap:\n",
    "                extra_needed = self.frame_cap - len(data)\n",
    "                extra_array = np.zeros((extra_needed, data.shape[1], 3), dtype=float)\n",
    "                data = np.concatenate((data, extra_array))\n",
    "        print(\"Before flatten keypoints X coord shape:\", data.shape)  # (87, 478, 3)  # 87*478*3 = 124758\n",
    "        data = data.reshape(self.frame_cap*self.keypoints_quantity * self.coordinate_amount)\n",
    "        print(\"After flattening keypoints X coord shape:\", data.shape)\n",
    "                \n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d02a0e9",
   "metadata": {},
   "source": [
    "# Model Definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cffee87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class LSTM_DeceptionRecognition(nn.Module):\n",
    "    def __init__(self, lstm_embedding_length, lstm_size, lstm_layers, inner_linear_size, target_size):\n",
    "        super(LSTM_DeceptionRecognition, self).__init__()\n",
    "        \n",
    "        # var definition:\n",
    "        self.lstm_size = lstm_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        \n",
    "        # defining layers:\n",
    "        self.lstm = nn.LSTM(lstm_embedding_length, lstm_size, lstm_layers, batch_first=True).double()\n",
    "        self.inner_layer = nn.Linear(lstm_size, inner_linear_size)\n",
    "        self.label_layer = nn.Linear(inner_linear_size, target_size)\n",
    "        \n",
    "    def init_state(self, batch_size):\n",
    "        return (torch.zeros(self.lstm_layers, batch_size, self.lstm_size),\n",
    "                torch.zeros(self.lstm_layers, batch_size, self.lstm_size))\n",
    "\n",
    "    def forward(self, padded_input):\n",
    "        hs = self.init_state(self.lstm_size)\n",
    "        lstm_out, lstm_hidden_embedding = self.lstm(padded_input, hs)\n",
    "        inner_layer_out = self.inner_layer(lstm_out)\n",
    "        label_out = self.label_layer(inner_layer_out)\n",
    "        \n",
    "        return label_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a0ce7a",
   "metadata": {},
   "source": [
    "# Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a97a7c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame_sum: 42201.45619380613\n",
      "Input dimension set to: 124758\n",
      "Before flatten keypoints X coord shape: (87, 478, 3)\n",
      "After flattening keypoints X coord shape: (124758,)\n",
      "Before flatten keypoints X coord shape: (87, 478, 3)\n",
      "After flattening keypoints X coord shape: (124758,)\n",
      "Before flatten keypoints X coord shape: (87, 478, 3)\n",
      "After flattening keypoints X coord shape: (124758,)\n",
      "Before flatten keypoints X coord shape: (87, 478, 3)\n",
      "After flattening keypoints X coord shape: (124758,)\n",
      "Label: tensor([0, 1, 0, 1])\n",
      "Input data: torch.Size([4, 124758])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "For unbatched 2-D input, hx and cx should also be 2-D but got (3-D, 3-D) tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel:\u001b[39m\u001b[38;5;124m\"\u001b[39m, tmp_label)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput data:\u001b[39m\u001b[38;5;124m\"\u001b[39m, tmp_data\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 42\u001b[0m lstm_tmp_result \u001b[38;5;241m=\u001b[39m \u001b[43mlstm_caut_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(lstm_tmp_result)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\caut_training\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[20], line 25\u001b[0m, in \u001b[0;36mLSTM_DeceptionRecognition.forward\u001b[1;34m(self, padded_input)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, padded_input):\n\u001b[0;32m     24\u001b[0m     hs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm_size)\n\u001b[1;32m---> 25\u001b[0m     lstm_out, lstm_hidden_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadded_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     inner_layer_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minner_layer(lstm_out)\n\u001b[0;32m     27\u001b[0m     label_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_layer(inner_layer_out)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\caut_training\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\caut_training\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:765\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    762\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    763\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor unbatched 2-D input, hx and cx should \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    764\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malso be 2-D but got (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D) tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 765\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[0;32m    766\u001b[0m         hx \u001b[38;5;241m=\u001b[39m (hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    768\u001b[0m \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[0;32m    769\u001b[0m \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: For unbatched 2-D input, hx and cx should also be 2-D but got (3-D, 3-D) tensors"
     ]
    }
   ],
   "source": [
    "# setup dataset:\n",
    "deception_dataset = DeceptionDataset(mode=\"train\",\n",
    "                                     collection_type=\"MediaPipe\",\n",
    "                                     data_dir=os.path.join(os.getcwd(), 'mediaPipe_keypoints_data_UPD'),\n",
    "                                     csv_dir='../data',  # audio_data_UPD,\n",
    "                                     class_to_num={\"truth\": 0, \"lie\": 1},\n",
    "                                     num_to_class={0: \"truth\", 1: \"lie\"},\n",
    "                                     transform=None,\n",
    "                                     seconds_input_size=3,\n",
    "                                     fps_min=29,\n",
    "                                     keypoints_quantity=478,\n",
    "                                     coordinate_amount=3)\n",
    "\n",
    "\n",
    "# derive dataloader from deception_dataset:\n",
    "caut_dataloader = DataLoader(deception_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "\n",
    "# get lstm input size:\n",
    "input_dim = deception_dataset.input_dim\n",
    "print(\"Input dimension set to:\", input_dim)\n",
    "\n",
    "\n",
    "# initializing model:\n",
    "lstm_caut_model = LSTM_DeceptionRecognition(lstm_embedding_length=input_dim,\n",
    "                                            lstm_size=32,\n",
    "                                            lstm_layers=3,\n",
    "                                            inner_linear_size=50,\n",
    "                                            target_size=1)\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.Adam(lstm_caut_model.parameters(), lr=0.1)\n",
    "\n",
    "\n",
    "\n",
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    for i_batch, batch_data in enumerate(caut_dataloader):\n",
    "        tmp_data, tmp_label = batch_data\n",
    "        print(\"Label:\", tmp_label)\n",
    "        print(\"Input data:\", tmp_data.shape)\n",
    "        lstm_tmp_result = lstm_caut_model(tmp_data)\n",
    "        print(lstm_tmp_result)\n",
    "        break\n",
    "\n",
    "    \n",
    "'''\n",
    "for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# See what the scores are after training\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "\n",
    "    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "    # for word i. The predicted tag is the maximum scoring tag.\n",
    "    # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "    # since 0 is index of the maximum value of row 1,\n",
    "    # 1 is the index of maximum value of row 2, etc.\n",
    "    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "    print(tag_scores)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c3d10c",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabc4b2b",
   "metadata": {},
   "source": [
    "# Experimental Area:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85974818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.array([[[1,1,1],[2,2,2]], [[3,3,3],[4,4,4]]]).shape)\n",
    "# np.repeat(np.array([[[1,1,1],[2,2,2]], [[3,3,3],[4,4,4]]]), 2, axis=1).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
