{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80b08d3e",
   "metadata": {},
   "source": [
    "# DeepFace Review:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd10f51d",
   "metadata": {},
   "source": [
    "### These models DeepFace uses in the background are:\n",
    "* VGG-Face\n",
    "* Google FaceNet\n",
    "* OpenFace\n",
    "* Facebook DeepFace\n",
    "* DeepID\n",
    "* ArcFace\n",
    "* Dlib\n",
    "\n",
    "### These models are so good that they have demonstrated that they can analyze images of faces (and even videos) at a level that surpasses what is humanly possible. The face recognition pipeline of DeepFace consists of four stages:\n",
    "* Detection \n",
    "* Alignment\n",
    "* Representation \n",
    "* Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1222f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting deepface\n",
      "  Downloading deepface-0.0.79-py3-none-any.whl (49 kB)\n",
      "     ---------------------------------------- 49.6/49.6 kB 2.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas>=0.23.4 in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from deepface) (1.5.3)\n",
      "Collecting tensorflow>=1.9.0\n",
      "  Downloading tensorflow-2.11.0-cp310-cp310-win_amd64.whl (1.9 kB)\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from deepface) (1.24.2)\n",
      "Requirement already satisfied: tqdm>=4.30.0 in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from deepface) (4.64.1)\n",
      "Requirement already satisfied: opencv-python>=4.5.5.64 in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from deepface) (4.7.0.72)\n",
      "Collecting keras>=2.2.0\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 1.1 MB/s eta 0:00:00\n",
      "Collecting retina-face>=0.0.1\n",
      "  Downloading retina_face-0.0.13-py3-none-any.whl (16 kB)\n",
      "Collecting mtcnn>=0.1.0\n",
      "  Downloading mtcnn-0.1.1-py3-none-any.whl (2.3 MB)\n",
      "     ---------------------------------------- 2.3/2.3 MB 935.2 kB/s eta 0:00:00\n",
      "Requirement already satisfied: Pillow>=5.2.0 in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from deepface) (9.4.0)\n",
      "Collecting fire>=0.4.0\n",
      "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
      "     ---------------------------------------- 88.3/88.3 kB 1.3 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting Flask>=1.1.2\n",
      "  Downloading Flask-2.2.3-py3-none-any.whl (101 kB)\n",
      "     -------------------------------------- 101.8/101.8 kB 1.9 MB/s eta 0:00:00\n",
      "Collecting gunicorn>=20.1.0\n",
      "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
      "     ---------------------------------------- 79.5/79.5 kB 2.2 MB/s eta 0:00:00\n",
      "Collecting gdown>=3.10.1\n",
      "  Downloading gdown-4.6.4-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: six in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from fire>=0.4.0->deepface) (1.16.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
      "Requirement already satisfied: Jinja2>=3.0 in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from Flask>=1.1.2->deepface) (3.1.2)\n",
      "Collecting click>=8.0\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "     ---------------------------------------- 96.6/96.6 kB 1.4 MB/s eta 0:00:00\n",
      "Collecting itsdangerous>=2.0\n",
      "  Downloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
      "Collecting Werkzeug>=2.2.2\n",
      "  Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
      "     -------------------------------------- 233.6/233.6 kB 1.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from gdown>=3.10.1->deepface) (2.28.2)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.9.0-py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from gdown>=3.10.1->deepface) (4.11.2)\n",
      "Requirement already satisfied: setuptools>=3.0 in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from gunicorn>=20.1.0->deepface) (65.6.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from pandas>=0.23.4->deepface) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from pandas>=0.23.4->deepface) (2.8.2)\n",
      "Collecting tensorflow-intel==2.11.0\n",
      "  Downloading tensorflow_intel-2.11.0-cp310-cp310-win_amd64.whl (266.3 MB)\n",
      "     ------------------------------------ 266.3/266.3 MB 928.1 kB/s eta 0:00:00\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow>=1.9.0->deepface) (23.1.21)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     ---------------------------------------- 57.5/57.5 kB 1.0 MB/s eta 0:00:00\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 891.3 kB/s eta 0:00:00\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     -------------------------------------- 65.5/65.5 kB 891.1 kB/s eta 0:00:00\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp310-cp310-win_amd64.whl (895 kB)\n",
      "     ------------------------------------ 895.7/895.7 kB 776.7 kB/s eta 0:00:00\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.51.3-cp310-cp310-win_amd64.whl (3.7 MB)\n",
      "     ---------------------------------------- 3.7/3.7 MB 1.1 MB/s eta 0:00:00\n",
      "Collecting tensorboard<2.12,>=2.11\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "     ---------------------------------------- 6.0/6.0 MB 943.0 kB/s eta 0:00:00\n",
      "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "     -------------------------------------- 439.2/439.2 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.15.0-cp310-cp310-win_amd64.whl (36 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\lrspr\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.11.0->tensorflow>=1.9.0->deepface) (1.3.0)\n",
      "Collecting typing-extensions>=3.6.6\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.8.0-cp310-cp310-win_amd64.whl (2.6 MB)\n",
      "     ---------------------------------------- 2.6/2.6 MB 838.2 kB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow>=1.9.0->deepface) (23.0)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-15.0.6.1-py2.py3-none-win_amd64.whl (23.2 MB)\n",
      "     -------------------------------------- 23.2/23.2 MB 882.2 kB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from tqdm>=4.30.0->deepface) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from Jinja2>=3.0->Flask>=1.1.2->deepface) (2.1.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from beautifulsoup4->gdown>=3.10.1->deepface) (2.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from requests[socks]->gdown>=3.10.1->deepface) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from requests[socks]->gdown>=3.10.1->deepface) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from requests[socks]->gdown>=3.10.1->deepface) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from requests[socks]->gdown>=3.10.1->deepface) (3.4)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\lrspr\\anaconda3\\envs\\caut_env\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow>=1.9.0->deepface) (0.38.4)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "     -------------------------------------- 781.3/781.3 kB 1.0 MB/s eta 0:00:00\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "     ---------------------------------------- 93.3/93.3 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.16.2-py2.py3-none-any.whl (177 kB)\n",
      "     -------------------------------------- 177.2/177.2 kB 1.5 MB/s eta 0:00:00\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "     -------------------------------------- 155.3/155.3 kB 1.0 MB/s eta 0:00:00\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "     -------------------------------------- 77.1/77.1 kB 846.9 kB/s eta 0:00:00\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     ------------------------------------ 151.7/151.7 kB 645.9 kB/s eta 0:00:00\n",
      "Building wheels for collected packages: fire\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116947 sha256=d2e5328a6ffaae8e6a1e3085fe431534112cb4ae776650b9ccda31d5029a6b04\n",
      "  Stored in directory: C:\\Users\\lrspr\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-kiyz_7b8\\wheels\\c4\\eb\\6a\\1c6d2ad660043768e998bdf9c6a28db2f1b7db3a5825d51e87\n",
      "Successfully built fire\n",
      "Installing collected packages: tensorboard-plugin-wit, pyasn1, libclang, wrapt, Werkzeug, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, PySocks, pyasn1-modules, protobuf, opt-einsum, oauthlib, markdown, keras, itsdangerous, h5py, gunicorn, grpcio, google-pasta, gast, filelock, click, cachetools, astunparse, requests-oauthlib, mtcnn, google-auth, Flask, fire, google-auth-oauthlib, gdown, tensorboard, tensorflow-intel, tensorflow, retina-face, deepface\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\lrspr\\\\anaconda3\\\\envs\\\\caut_env\\\\Lib\\\\site-packages\\\\google\\\\~rotobuf\\\\internal\\\\_api_implementation.cp310-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install deepface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2cec606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e900d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "race_model_single_batch.h5 will be downloaded...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://github.com/serengil/deepface_models/releases/download/v1.0/race_model_single_batch.h5\n",
      "To: C:\\Users\\lrspr\\.deepface\\weights\\race_model_single_batch.h5\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 537M/537M [00:14<00:00, 37.7MB/s]\n",
      "Action: race: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face_analysis for frame {frame_count}: [{'emotion': {'angry': 0.16445438377559185, 'disgust': 0.020809698617085814, 'fear': 48.508599400520325, 'happy': 0.00018318594356969697, 'sad': 51.29292011260986, 'surprise': 9.269933798350394e-06, 'neutral': 0.013024600048083812}, 'dominant_emotion': 'sad', 'region': {'x': 6, 'y': 10, 'w': 117, 'h': 117}, 'age': 22, 'gender': {'Woman': 0.019238154345657676, 'Man': 99.98076558113098}, 'dominant_gender': 'Man', 'race': {'asian': 4.685890302062035, 'indian': 15.215986967086792, 'black': 1.988225057721138, 'white': 19.47016716003418, 'middle eastern': 38.63554298877716, 'latino hispanic': 20.004186034202576}, 'dominant_race': 'middle eastern'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  6.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face_analysis for frame {frame_count}: [{'emotion': {'angry': 63.53838282366004, 'disgust': 6.986252165504953e-06, 'fear': 28.710975800636316, 'happy': 0.08826407832298239, 'sad': 6.554166191319681, 'surprise': 0.00021591171998808844, 'neutral': 1.1079857667701523}, 'dominant_emotion': 'angry', 'region': {'x': 28, 'y': 18, 'w': 82, 'h': 82}, 'age': 22, 'gender': {'Woman': 0.8673837408423424, 'Man': 99.13261532783508}, 'dominant_gender': 'Man', 'race': {'asian': 24.71116034455063, 'indian': 3.4909630412437345, 'black': 0.7215829064895455, 'white': 38.831653525331824, 'middle eastern': 10.651949682947974, 'latino hispanic': 21.592695807975282}, 'dominant_race': 'white'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face_analysis for frame {frame_count}: [{'emotion': {'angry': 0.5664533004164696, 'disgust': 0.00042275200939911883, 'fear': 0.9944024495780468, 'happy': 95.0090229511261, 'sad': 1.5692507848143578, 'surprise': 6.648767092443109e-08, 'neutral': 1.860448345541954}, 'dominant_emotion': 'happy', 'region': {'x': 16, 'y': 9, 'w': 108, 'h': 108}, 'age': 24, 'gender': {'Woman': 0.021049194037914276, 'Man': 99.97895359992981}, 'dominant_gender': 'Man', 'race': {'asian': 5.457611382007599, 'indian': 3.8705047219991684, 'black': 0.3328523598611355, 'white': 47.75943160057068, 'middle eastern': 27.54383385181427, 'latino hispanic': 15.035766363143921}, 'dominant_race': 'white'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  6.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face_analysis for frame {frame_count}: [{'emotion': {'angry': 0.003238333203832922, 'disgust': 7.425792193004524e-05, 'fear': 0.40085783754931914, 'happy': 6.487934843056153e-06, 'sad': 99.59446189370524, 'surprise': 9.496948238778031e-08, 'neutral': 0.0013610525358653276}, 'dominant_emotion': 'sad', 'region': {'x': 7, 'y': 14, 'w': 112, 'h': 112}, 'age': 24, 'gender': {'Woman': 0.027011553174816072, 'Man': 99.97298121452332}, 'dominant_gender': 'Man', 'race': {'asian': 5.094516277313232, 'indian': 11.26137599349022, 'black': 0.966689083725214, 'white': 24.360056221485138, 'middle eastern': 39.53970670700073, 'latino hispanic': 18.7776580452919}, 'dominant_race': 'middle eastern'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face_analysis for frame {frame_count}: [{'emotion': {'angry': 0.4516325963744636, 'disgust': 7.786516467235094e-05, 'fear': 0.0928959978191999, 'happy': 68.25441339664619, 'sad': 0.46200203628862335, 'surprise': 0.00016273221156519542, 'neutral': 30.738816349365276}, 'dominant_emotion': 'happy', 'region': {'x': 13, 'y': 8, 'w': 106, 'h': 106}, 'age': 23, 'gender': {'Woman': 0.023082912957761437, 'Man': 99.97691512107849}, 'dominant_gender': 'Man', 'race': {'asian': 1.1109860220866177, 'indian': 1.071441045827741, 'black': 0.04227985706010675, 'white': 68.41239113659964, 'middle eastern': 23.063574962189797, 'latino hispanic': 6.2993228376718955}, 'dominant_race': 'white'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face_analysis for frame {frame_count}: [{'emotion': {'angry': 5.502072304610747, 'disgust': 3.280994089912591e-05, 'fear': 4.4230121030218665, 'happy': 88.49498560994778, 'sad': 1.5646570363436363, 'surprise': 6.503586390714394e-08, 'neutral': 0.015243386768807427}, 'dominant_emotion': 'happy', 'region': {'x': 9, 'y': 13, 'w': 106, 'h': 106}, 'age': 23, 'gender': {'Woman': 0.025238297530449927, 'Man': 99.97475743293762}, 'dominant_gender': 'Man', 'race': {'asian': 0.4686873219869925, 'indian': 0.9202726733847065, 'black': 0.029513640342161973, 'white': 71.16716332239886, 'middle eastern': 21.714481515555473, 'latino hispanic': 5.699886348836801}, 'dominant_race': 'white'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  6.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face_analysis for frame {frame_count}: [{'emotion': {'angry': 0.0011990005597503127, 'disgust': 2.0278186659797956e-11, 'fear': 58.83839831809978, 'happy': 0.809779766622754, 'sad': 31.966436739735215, 'surprise': 1.8944446049822712e-07, 'neutral': 8.38419373443052}, 'dominant_emotion': 'fear', 'region': {'x': 10, 'y': 12, 'w': 111, 'h': 111}, 'age': 24, 'gender': {'Woman': 0.01526308769825846, 'Man': 99.98472929000854}, 'dominant_gender': 'Man', 'race': {'asian': 0.7022295612841845, 'indian': 1.5377165749669075, 'black': 0.02637903962749988, 'white': 63.223785161972046, 'middle eastern': 29.591837525367737, 'latino hispanic': 4.91805300116539}, 'dominant_race': 'white'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face_analysis for frame {frame_count}: [{'emotion': {'angry': 0.010752374423343654, 'disgust': 4.240467171327364e-10, 'fear': 30.44931772049175, 'happy': 0.2761572531182917, 'sad': 68.33419611061788, 'surprise': 2.6155001946677164e-08, 'neutral': 0.9295776109145648}, 'dominant_emotion': 'sad', 'region': {'x': 11, 'y': 9, 'w': 110, 'h': 110}, 'age': 24, 'gender': {'Woman': 0.011281771003268659, 'Man': 99.9887228012085}, 'dominant_gender': 'Man', 'race': {'asian': 3.2010240525617633, 'indian': 1.7239219605280138, 'black': 0.05747891606637335, 'white': 62.94852274349467, 'middle eastern': 27.08708209318412, 'latino hispanic': 4.981972574113165}, 'dominant_race': 'white'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face_analysis for frame {frame_count}: [{'emotion': {'angry': 0.019039519247598946, 'disgust': 9.448916800901264e-12, 'fear': 70.62098979949951, 'happy': 11.408849060535431, 'sad': 17.42592304944992, 'surprise': 2.9850530380848284e-08, 'neutral': 0.5252011585980654}, 'dominant_emotion': 'fear', 'region': {'x': 8, 'y': 8, 'w': 111, 'h': 111}, 'age': 24, 'gender': {'Woman': 0.028737500542774796, 'Man': 99.97126460075378}, 'dominant_gender': 'Man', 'race': {'asian': 1.2427739608451105, 'indian': 0.7147054613951124, 'black': 0.021811974720066768, 'white': 76.39000871121935, 'middle eastern': 17.91949229952647, 'latino hispanic': 3.711205569577549}, 'dominant_race': 'white'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  6.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face_analysis for frame {frame_count}: [{'emotion': {'angry': 0.07345395279116929, 'disgust': 6.801212371065901e-09, 'fear': 33.289867639541626, 'happy': 2.7176694944500923, 'sad': 29.712364077568054, 'surprise': 2.190762948828251e-06, 'neutral': 34.20664668083191}, 'dominant_emotion': 'neutral', 'region': {'x': 16, 'y': 7, 'w': 106, 'h': 106}, 'age': 24, 'gender': {'Woman': 0.006502530595753342, 'Man': 99.99349117279053}, 'dominant_gender': 'Man', 'race': {'asian': 0.8710547350347042, 'indian': 4.090525209903717, 'black': 0.11234348639845848, 'white': 36.22295260429382, 'middle eastern': 49.76094663143158, 'latino hispanic': 8.942172676324844}, 'dominant_race': 'middle eastern'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  7.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face_analysis for frame {frame_count}: [{'emotion': {'angry': 0.00023331012042964363, 'disgust': 5.744577465421513e-10, 'fear': 2.786562385354184, 'happy': 3.6740625604428385, 'sad': 63.21024079383855, 'surprise': 1.682710848815828e-07, 'neutral': 30.328901429706896}, 'dominant_emotion': 'sad', 'region': {'x': 15, 'y': 8, 'w': 106, 'h': 106}, 'age': 24, 'gender': {'Woman': 0.019204798445571214, 'Man': 99.98080134391785}, 'dominant_gender': 'Man', 'race': {'asian': 6.986675411462784, 'indian': 5.815142393112183, 'black': 0.20114597864449024, 'white': 38.47131431102753, 'middle eastern': 39.35358226299286, 'latino hispanic': 9.172140061855316}, 'dominant_race': 'middle eastern'}]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "from deepface import DeepFace\n",
    "    \n",
    "cap = cv2.VideoCapture(0)\n",
    "pTime = 0\n",
    " \n",
    "mpFaceDetection = mp.solutions.face_detection\n",
    "mpDraw = mp.solutions.drawing_utils\n",
    "faceDetection = mpFaceDetection.FaceDetection(0.75)\n",
    "\n",
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    \n",
    "    try:\n",
    " \n",
    "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        results = faceDetection.process(imgRGB)\n",
    "\n",
    "        if results.detections:\n",
    "            for id, detection in enumerate(results.detections):\n",
    "                # mpDraw.draw_detection(img, detection)\n",
    "                # print(id, detection)\n",
    "                # print(detection.score)\n",
    "                # print(detection.location_data.relative_bounding_box)\n",
    "                bboxC = detection.location_data.relative_bounding_box\n",
    "                ih, iw, ic = img.shape\n",
    "                bbox = int(bboxC.xmin * iw), int(bboxC.ymin * ih), \\\n",
    "                       int(bboxC.width * iw), int(bboxC.height * ih)\n",
    "                y_start, y_end = bbox[1], (bbox[1]+bbox[3])\n",
    "                x_start, x_end = bbox[0], (bbox[0]+bbox[2])\n",
    "                face_cropped = img[y_start:y_end, x_start:x_end]\n",
    "                cv2.rectangle(img, bbox, (255, 0, 255), 2)\n",
    "                face_analysis = DeepFace.analyze(face_cropped)\n",
    "                print(\"face_analysis for frame {frame_count}:\", face_analysis)\n",
    "                frame_count+=1\n",
    "                # cv2.putText(img, f'{int(detection.score[0] * 100)}%',\n",
    "                #             (bbox[0], bbox[1] - 20), cv2.FONT_HERSHEY_PLAIN,\n",
    "                #             2, (255, 0, 255), 2)\n",
    "\n",
    "        cTime = time.time()\n",
    "        fps = 1 / (cTime - pTime)\n",
    "        pTime = cTime\n",
    "        # cv2.putText(img, f'FPS: {int(fps)}', (20, 70), cv2.FONT_HERSHEY_PLAIN,\n",
    "        #             3, (0, 255, 0), 2)\n",
    "        cv2.imshow(\"Image\", face_cropped)\n",
    "        cv2.waitKey(1)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a47400",
   "metadata": {},
   "source": [
    "# DeepFace insights:\n",
    "### Does not provide Action Units, but only provides emotions instead. This is not sufficient for our use. Furthermore, the FPS when running with DeepFace is very low.\n",
    "\n",
    "### >>> Thus, DeepFace module will not be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02a3cc6",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23ff904",
   "metadata": {},
   "source": [
    "# Let's map each  MediaPipe coordinate over face to see what keypoints represent what features on the face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f17ca61",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mediapipe'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageFont, ImageDraw, Image, ImageOps\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mediapipe'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import mediapipe as mp\n",
    "from PIL import ImageFont, ImageDraw, Image, ImageOps\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def align_face(img):\n",
    "    pass\n",
    "    \n",
    "def plot_coordinates(verbose=True):\n",
    "    \n",
    "    detected_keypoints_coordinates = []\n",
    "    font = ImageFont.truetype(\"arial.ttf\", 10)\n",
    "    \n",
    "    img = Image.open(\"img_sample.jpg\")\n",
    "    plt.imshow(img)\n",
    "    print(\"img.shape:\", img)\n",
    "    img = ImageOps.fit(img, (2040,2040))\n",
    "    img = np.asarray(img)\n",
    "    print(\"img.shape:\", img.shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # Media Pipe Initialization:\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    mp_drawing_styles = mp.solutions.drawing_styles\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "                               max_num_faces=1,\n",
    "                               refine_landmarks=True,\n",
    "                               min_detection_confidence=0.5,\n",
    "                               min_tracking_confidence=0.5) as face_mesh:\n",
    "        img.flags.writeable = False\n",
    "        image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(image)\n",
    "        \n",
    "        total_count = 0\n",
    "        \n",
    "        \n",
    "        #####################################################\n",
    "        if results.multi_face_landmarks:\n",
    "            current_coordinate_set = []\n",
    "            for face in results.multi_face_landmarks:\n",
    "                for landmark in face.landmark:\n",
    "                    x = landmark.x\n",
    "                    y = landmark.y\n",
    "                    z = landmark.z\n",
    "                    \n",
    "                    current_coordinate_set.append([x,y,z])\n",
    "\n",
    "                    if verbose:\n",
    "                        shape = image.shape \n",
    "                        relative_x = int(x * shape[1])\n",
    "                        relative_y = int(y * shape[0])\n",
    "\n",
    "                        # cv2.circle(image, (relative_x, relative_y), radius=1, color=(225, 0, 100), thickness=1)\n",
    "                        # cv2.putText(image, f\"{total_count}\", (relative_x, relative_y), cv2.FONT_HERSHEY_SIMPLEX, 0.01, 120)\n",
    "                        pil_im = Image.fromarray(image)\n",
    "                        draw = ImageDraw.Draw(pil_im)\n",
    "                        # Choose a font\n",
    "                        # font = ImageFont.truetype(\"Roboto-Regular.ttf\", 50)\n",
    "                        # Draw the text\n",
    "                        draw.text((relative_x, relative_y), f\"{total_count}\", font=font)  # , font=font)\n",
    "                        image = cv2_im_processed = cv2.cvtColor(np.array(pil_im), cv2.COLOR_RGB2BGR)\n",
    "                    total_count+=1\n",
    "        else:\n",
    "            # print(traceback.format_exc())\n",
    "            x, y, z = 0.0, 0.0, 0.0\n",
    "            current_coordinate_set.append([x,y,z]*478)\n",
    "        \n",
    "        print(\"total_count:\", total_count)\n",
    "            \n",
    "        # Append detected_keypoints_coordinates:\n",
    "        detected_keypoints_coordinates.append(current_coordinate_set)               \n",
    "\n",
    "        # if verbose:\n",
    "        #    pass  # cv2.imshow('MediaPipe Face Mesh', cv2.flip(image, 1))\n",
    "        # plt.figure()  # figsize=(15,15))\n",
    "        # plt.imshow(image)\n",
    "        # plt.savefig('mediaPipe_coordinates_location.png')\n",
    "        cv2.imwrite('mediaPipe_coordinates_location.png', image)\n",
    "        plt.figure()\n",
    "        plt.imshow(image)\n",
    "        #####################################################\n",
    "    \n",
    "    \n",
    "plot_coordinates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327ff4e7",
   "metadata": {},
   "source": [
    "# Face Crop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e7d49aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 2.],\n",
       "       [0., 1., 2.],\n",
       "       [0., 1., 2.],\n",
       "       [0., 1., 2.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = np.array([np.array([0.0,1.0,2.0])]*4)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e320ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-10., -19.,   2.],\n",
       "       [-10., -19.,   2.],\n",
       "       [-10., -19.,   2.],\n",
       "       [-10., -19.,   2.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[:,0]-=10\n",
    "tmp[:,1]-=20\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eef7f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp_set = [np.array([1,2,3]), np.array([1,2,3]), np.array([1,2,3]), np.array([1,2,3])]\n",
    "# add_more_tmp = [np.array([0.0, 0.0, 0.0])] * (5 - len(tmp_set))\n",
    "# tmp_set += add_more_tmp\n",
    "# tmp_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2dc945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import mediapipe as mp\n",
    "from PIL import ImageFont, ImageDraw, Image\n",
    "import numpy as np\n",
    "import math\n",
    "import traceback\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "\n",
    "\n",
    "\n",
    "def align_face_and_store_coords(img, aligned_face):\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # Media Pipe Initialization:\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    mp_drawing_styles = mp.solutions.drawing_styles\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "                               max_num_faces=1,\n",
    "                               refine_landmarks=True,\n",
    "                               min_detection_confidence=0.5,\n",
    "                               min_tracking_confidence=0.5) as face_mesh:\n",
    "        img.flags.writeable = False\n",
    "        image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(image)\n",
    "\n",
    "        coordinate_count = 0\n",
    "\n",
    "        #####################################################\n",
    "        if results.multi_face_landmarks:\n",
    "            for face in results.multi_face_landmarks:\n",
    "                \n",
    "                current_coordinate_set = []\n",
    "                \n",
    "                # eye coordinate storage for current face:\n",
    "                eye_dictionary = {}\n",
    "                for landmark in face.landmark:\n",
    "                    x = landmark.x\n",
    "                    y = landmark.y\n",
    "                    z = landmark.z\n",
    "\n",
    "                    current_coordinate_set.append(np.array([x,y,z]))\n",
    "\n",
    "                    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "                    # get data for face alignment\n",
    "                    shape = image.shape \n",
    "                    relative_x = int(x * shape[1])\n",
    "                    relative_y = int(y * shape[0])\n",
    "\n",
    "                    # 468 eye from our left, 473 eye from our right.\n",
    "                    if coordinate_count==468 or coordinate_count==473:\n",
    "                        # Draw eyes:\n",
    "                        # cv2.circle(image, (relative_x, relative_y), radius=1, color=(225, 255, 255), thickness=1)\n",
    "                        # storing data for that:\n",
    "                        eye_dictionary[coordinate_count] = [relative_x, relative_y]\n",
    "                    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "\n",
    "                    # keep track of current coordinate item\n",
    "                    coordinate_count+=1\n",
    "                \n",
    "                ####################################################################################################\n",
    "                if len(current_coordinate_set) < 478:\n",
    "                    add_more = [np.array([0.0, 0.0, 0.0])] * (478 - len(current_coordinate_set))\n",
    "                    current_coordinate_set += add_more\n",
    "                current_coordinate_set = current_coordinate_set[:478]\n",
    "                ####################################################################################################\n",
    "                \n",
    "\n",
    "                # calculating face alignment (https://www.youtube.com/watch?v=WA9i68g4meI&ab_channel=SefikIlkinSerengil):\n",
    "                x_right_eye, y_right_eye = eye_dictionary[468][0], eye_dictionary[468][1]\n",
    "                x_left_eye,  y_left_eye  = eye_dictionary[473][0], eye_dictionary[473][1]\n",
    "\n",
    "                a = abs(y_right_eye-y_left_eye)\n",
    "                b = abs(x_left_eye-x_right_eye)\n",
    "                c = math.sqrt(a*a + b*b)\n",
    "                # print(f\"a={a}, b={b}, c={c}\")\n",
    "\n",
    "                cos_alpha = (b*b + c*c - a*a) / (2*b*c)\n",
    "                # print(f\"cos_alpha={cos_alpha}\")\n",
    "\n",
    "                alpha = np.arccos(cos_alpha)  # radius\n",
    "                alpha = (alpha*180) / math.pi\n",
    "\n",
    "                direction = 1\n",
    "                if y_left_eye < y_right_eye:\n",
    "                    direction = -1\n",
    "                else:\n",
    "                    direction = 1\n",
    "\n",
    "                alpha = direction*alpha\n",
    "\n",
    "                # print(f\"alpha={alpha}\")\n",
    "\n",
    "                # finally, align it:\n",
    "                aligned_face = Image.fromarray(image)\n",
    "                aligned_face = np.array(aligned_face.rotate(alpha))[:, :, ::-1].copy()\n",
    "                \n",
    "                break  # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< do only one face.\n",
    "                \n",
    "        else:\n",
    "            current_coordinate_set = []\n",
    "            x, y, z = 0.0, 0.0, 0.0\n",
    "            current_coordinate_set.append([np.array([x,y,z])]*478)  # np.array([x,y,z])*478)\n",
    "                \n",
    "    return aligned_face, np.array(current_coordinate_set)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def detect_face_mesh_coords(img):\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # Media Pipe Initialization:\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    mp_drawing_styles = mp.solutions.drawing_styles\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "                               max_num_faces=1,\n",
    "                               refine_landmarks=True,\n",
    "                               min_detection_confidence=0.5,\n",
    "                               min_tracking_confidence=0.5) as face_mesh:\n",
    "        img.flags.writeable = False\n",
    "        image = img  # cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(image)\n",
    "\n",
    "        coordinate_count = 0\n",
    "\n",
    "        #####################################################\n",
    "        if results.multi_face_landmarks:\n",
    "            for face in results.multi_face_landmarks:\n",
    "                \n",
    "                current_coordinate_set = []\n",
    "                \n",
    "                # eye coordinate storage for current face:\n",
    "                eye_dictionary = {}\n",
    "                for landmark in face.landmark:\n",
    "                    x = landmark.x\n",
    "                    y = landmark.y\n",
    "                    z = landmark.z\n",
    "\n",
    "                    current_coordinate_set.append(np.array([x,y,z]))\n",
    "\n",
    "                    # keep track of current coordinate item\n",
    "                    coordinate_count+=1\n",
    "                \n",
    "                if len(current_coordinate_set) < 478:\n",
    "                    add_more = [np.array([0.0, 0.0, 0.0])] * (478 - len(current_coordinate_set))\n",
    "                    current_coordinate_set += add_more\n",
    "                current_coordinate_set = current_coordinate_set[:478]\n",
    "                \n",
    "                break  # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< do only one face.\n",
    "                \n",
    "        else:\n",
    "            current_coordinate_set = []\n",
    "            x, y, z = 0.0, 0.0, 0.0\n",
    "            current_coordinate_set.append([np.array([x,y,z])]*478)  # np.array([x,y,z])*478)\n",
    "                \n",
    "    return np.array(current_coordinate_set)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def image_resize(image, width = None, height = None, inter = cv2.INTER_AREA):\n",
    "    # initialize the dimensions of the image to be resized and\n",
    "    # grab the image size\n",
    "    dim = None\n",
    "    (h, w) = image.shape[:2]\n",
    "\n",
    "    # if both the width and height are None, then return the\n",
    "    # original image\n",
    "    if width is None and height is None:\n",
    "        return image\n",
    "\n",
    "    # check to see if the width is None\n",
    "    if width is None:\n",
    "        # calculate the ratio of the height and construct the\n",
    "        # dimensions\n",
    "        r = height / float(h)\n",
    "        dim = (int(w * r), height)\n",
    "\n",
    "    # otherwise, the height is None\n",
    "    else:\n",
    "        # calculate the ratio of the width and construct the\n",
    "        # dimensions\n",
    "        r = width / float(w)\n",
    "        dim = (width, int(h * r))\n",
    "\n",
    "    # resize the image\n",
    "    resized = cv2.resize(image, dim, interpolation = inter)\n",
    "\n",
    "    # return the resized image\n",
    "    return resized\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def face_crop():\n",
    "    \n",
    "    detected_keypoints_coordinates = []\n",
    "    \n",
    "    # img = cv2.imread(\"img_sample.jpg\")\n",
    "    # plt.imshow(img)    \n",
    "    aligned_face = None\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    with mp_face_detection.FaceDetection(\n",
    "        model_selection=0, min_detection_confidence=0.4) as face_detection:\n",
    "        while cap.isOpened():\n",
    "            success, image = cap.read()\n",
    "\n",
    "            try:\n",
    "\n",
    "                results = face_detection.process(image)\n",
    "\n",
    "                # Draw the face detection annotations on the image.\n",
    "                image.flags.writeable = True\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "                if results.detections:\n",
    "                    for detection in results.detections:\n",
    "                        #### here i want to grab the bounding box for the detected faces in order to crop the face image\n",
    "                        location_data = detection.location_data\n",
    "                        bb = location_data.relative_bounding_box\n",
    "                        bb_box = [\n",
    "                            int(bb.xmin*image.shape[1]), int(bb.ymin*image.shape[0]),\n",
    "                            int(bb.width*image.shape[1]), int(bb.height*image.shape[0]),\n",
    "                        ]\n",
    "                        \n",
    "                        # get coords:\n",
    "                        xmin = bb_box[0]\n",
    "                        xmax = xmin + bb_box[2]\n",
    "                        ymin = bb_box[1]\n",
    "                        ymax = ymin + bb_box[3]\n",
    "                        \n",
    "                        # Increase bbox:\n",
    "                        increase_by = 0.20\n",
    "                        xmin -= int(increase_by * (xmax - xmin))\n",
    "                        xmax += int(increase_by * (xmax - xmin))\n",
    "                        ymin -= int(increase_by * (ymax - ymin))\n",
    "                        ymax += int(increase_by * (ymax - ymin))\n",
    "                        \n",
    "                        # Face Crop with enlarged Bbox\n",
    "                        cropped_face = image[ymin:ymax, xmin:xmax]\n",
    "                        aligned_face, current_coordinate_set = align_face_and_store_coords(cropped_face, aligned_face)\n",
    "                        \n",
    "                        # DRAW:\n",
    "                        current_coordinate_set = detect_face_mesh_coords(aligned_face)\n",
    "                        for current_xyz_coord in current_coordinate_set:\n",
    "                            relative_x, relative_y, relative_z = current_xyz_coord[0], current_xyz_coord[1], current_xyz_coord[2]\n",
    "                            cv2.circle(aligned_face, (int(relative_x), int(relative_y)), radius=1, color=(225, 0, 100), thickness=1)\n",
    "                            # cv2.putText(aligned_face, f\"{total_count}\", (relative_x, relative_y), cv2.FONT_HERSHEY_SIMPLEX, 0.01, 120)\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                # Append detected_keypoints_coordinates:\n",
    "                detected_keypoints_coordinates.append(current_coordinate_set)\n",
    "                \n",
    "                if aligned_face is not None:\n",
    "                    aligned_face = image_resize(aligned_face, height = 640)\n",
    "                    cv2.imshow(\"Image\", aligned_face)\n",
    "                if cv2.waitKey(33) == ord('q'):\n",
    "                    break\n",
    "\n",
    "            except:\n",
    "                print(\"\\n***ERROR***\\n\")\n",
    "                print(traceback.format_exc())\n",
    "        \n",
    "        \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    \n",
    "face_crop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06796628",
   "metadata": {},
   "source": [
    "# Face Alignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a8415da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import mediapipe as mp\n",
    "from PIL import ImageFont, ImageDraw, Image\n",
    "import numpy as np\n",
    "import math\n",
    "import traceback\n",
    " \n",
    "    \n",
    "def align_face_and_store_coords(img, aligned_face):\n",
    "    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # Media Pipe Initialization:\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    mp_drawing_styles = mp.solutions.drawing_styles\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "                               max_num_faces=1,\n",
    "                               refine_landmarks=True,\n",
    "                               min_detection_confidence=0.5,\n",
    "                               min_tracking_confidence=0.5) as face_mesh:\n",
    "        img.flags.writeable = False\n",
    "        image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(image)\n",
    "\n",
    "        coordinate_count = 0\n",
    "\n",
    "\n",
    "        #####################################################\n",
    "        if results.multi_face_landmarks:\n",
    "            current_coordinate_set = []\n",
    "\n",
    "            for face in results.multi_face_landmarks:\n",
    "                # eye coordinate storage for current face:\n",
    "                eye_dictionary = {}\n",
    "                for landmark in face.landmark:\n",
    "                    x = landmark.x\n",
    "                    y = landmark.y\n",
    "                    z = landmark.z\n",
    "\n",
    "                    current_coordinate_set.append([x,y,z])\n",
    "\n",
    "                    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "                    # get data for face alignment\n",
    "                    shape = image.shape \n",
    "                    relative_x = int(x * shape[1])\n",
    "                    relative_y = int(y * shape[0])\n",
    "\n",
    "                    # 468 eye from our left, 473 eye from our right.\n",
    "                    if coordinate_count==468 or coordinate_count==473:\n",
    "                        cv2.circle(image, (relative_x, relative_y), radius=1, color=(225, 255, 255), thickness=1)\n",
    "                        # storing data for that:\n",
    "                        eye_dictionary[coordinate_count] = [relative_x, relative_y]\n",
    "                    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "\n",
    "                    # keep track of current coordinate item\n",
    "                    coordinate_count+=1\n",
    "\n",
    "                # calculating face alignment (https://www.youtube.com/watch?v=WA9i68g4meI&ab_channel=SefikIlkinSerengil):\n",
    "                x_right_eye, y_right_eye = eye_dictionary[468][0], eye_dictionary[468][1]\n",
    "                x_left_eye,  y_left_eye  = eye_dictionary[473][0], eye_dictionary[473][1]\n",
    "\n",
    "                a = abs(y_right_eye-y_left_eye)\n",
    "                b = abs(x_left_eye-x_right_eye)\n",
    "                c = math.sqrt(a*a + b*b)\n",
    "                # print(f\"a={a}, b={b}, c={c}\")\n",
    "\n",
    "                cos_alpha = (b*b + c*c - a*a) / (2*b*c)\n",
    "                # print(f\"cos_alpha={cos_alpha}\")\n",
    "\n",
    "                alpha = np.arccos(cos_alpha)  # radius\n",
    "                alpha = (alpha*180) / math.pi\n",
    "\n",
    "                direction = 1\n",
    "                if y_left_eye < y_right_eye:\n",
    "                    direction = -1\n",
    "                else:\n",
    "                    direction = 1\n",
    "\n",
    "                alpha = direction*alpha\n",
    "\n",
    "                # print(f\"alpha={alpha}\")\n",
    "\n",
    "                # finally, align it:\n",
    "                aligned_face = Image.fromarray(image)\n",
    "                aligned_face = np.array(aligned_face.rotate(alpha))[:, :, ::-1].copy()\n",
    "                \n",
    "        else:\n",
    "            # print(traceback.format_exc())\n",
    "            x, y, z = 0.0, 0.0, 0.0\n",
    "            current_coordinate_set.append([x,y,z]*478)\n",
    "                \n",
    "    return aligned_face, current_coordinate_set\n",
    "    \n",
    "    \n",
    "    \n",
    "def test_alignment(verbose=True):\n",
    "    \n",
    "    detected_keypoints_coordinates = []\n",
    "    \n",
    "    # img = cv2.imread(\"img_sample.jpg\")\n",
    "    # plt.imshow(img)    \n",
    "    aligned_face = None\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    while True:\n",
    "        success, img = cap.read()\n",
    "        \n",
    "        try:\n",
    "                \n",
    "            aligned_face, current_coordinate_set = align_face_and_store_coords(img, aligned_face)\n",
    "            \n",
    "            # Append detected_keypoints_coordinates:\n",
    "            detected_keypoints_coordinates.append(current_coordinate_set)         \n",
    "                \n",
    "                \n",
    "            if aligned_face is not None:\n",
    "                cv2.imshow(\"Image\", aligned_face)\n",
    "            if cv2.waitKey(33) == ord('q'):\n",
    "                break\n",
    "    \n",
    "        except:\n",
    "            print(\"\\n***ERROR***\\n\")\n",
    "            print(traceback.format_exc())\n",
    "            try:\n",
    "                if aligned_face is not None:\n",
    "                    print(aligned_face.shape)\n",
    "                print(\"*******\")\n",
    "            except:\n",
    "                pass\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "            \n",
    "    \n",
    "test_alignment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6ca4c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import mediapipe as mp\n",
    "from PIL import ImageFont, ImageDraw, Image\n",
    "import numpy as np\n",
    "import math\n",
    "import traceback\n",
    "\n",
    "\n",
    "def align_face(img):\n",
    "    pass\n",
    "    \n",
    "def test_alignment(verbose=True):\n",
    "    \n",
    "    detected_keypoints_coordinates = []\n",
    "    \n",
    "    # img = cv2.imread(\"img_sample.jpg\")\n",
    "    # plt.imshow(img)    \n",
    "    aligned_face = None\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    while True:\n",
    "        success, img = cap.read()\n",
    "        \n",
    "        try:\n",
    "    \n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "            # Media Pipe Initialization:\n",
    "            mp_drawing = mp.solutions.drawing_utils\n",
    "            mp_drawing_styles = mp.solutions.drawing_styles\n",
    "            mp_face_mesh = mp.solutions.face_mesh\n",
    "            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "            with mp_face_mesh.FaceMesh(\n",
    "                                       max_num_faces=1,\n",
    "                                       refine_landmarks=True,\n",
    "                                       min_detection_confidence=0.5,\n",
    "                                       min_tracking_confidence=0.5) as face_mesh:\n",
    "                img.flags.writeable = False\n",
    "                image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                results = face_mesh.process(image)\n",
    "\n",
    "                coordinate_count = 0\n",
    "\n",
    "\n",
    "                #####################################################\n",
    "                if results.multi_face_landmarks:\n",
    "                    current_coordinate_set = []\n",
    "        \n",
    "                    for face in results.multi_face_landmarks:\n",
    "                        # eye coordinate storage for current face:\n",
    "                        eye_dictionary = {}\n",
    "                        for landmark in face.landmark:\n",
    "                            x = landmark.x\n",
    "                            y = landmark.y\n",
    "                            z = landmark.z\n",
    "\n",
    "                            current_coordinate_set.append([x,y,z])\n",
    "\n",
    "                            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "                            # get data for face alignment\n",
    "                            shape = image.shape \n",
    "                            relative_x = int(x * shape[1])\n",
    "                            relative_y = int(y * shape[0])\n",
    "\n",
    "                            # 468 eye from our left, 473 eye from our right.\n",
    "                            if coordinate_count==468 or coordinate_count==473:\n",
    "                                cv2.circle(image, (relative_x, relative_y), radius=1, color=(225, 255, 255), thickness=1)\n",
    "                                # storing data for that:\n",
    "                                eye_dictionary[coordinate_count] = [relative_x, relative_y]\n",
    "                            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "                \n",
    "                                \n",
    "                            # keep track of current coordinate item\n",
    "                            coordinate_count+=1\n",
    "\n",
    "                        # calculating face alignment (https://www.youtube.com/watch?v=WA9i68g4meI&ab_channel=SefikIlkinSerengil):\n",
    "                        x_right_eye, y_right_eye = eye_dictionary[468][0], eye_dictionary[468][1]\n",
    "                        x_left_eye,  y_left_eye  = eye_dictionary[473][0], eye_dictionary[473][1]\n",
    "\n",
    "                        a = abs(y_right_eye-y_left_eye)\n",
    "                        b = abs(x_left_eye-x_right_eye)\n",
    "                        c = math.sqrt(a*a + b*b)\n",
    "                        # print(f\"a={a}, b={b}, c={c}\")\n",
    "\n",
    "                        cos_alpha = (b*b + c*c - a*a) / (2*b*c)\n",
    "                        # print(f\"cos_alpha={cos_alpha}\")\n",
    "\n",
    "                        alpha = np.arccos(cos_alpha)  # radius\n",
    "                        alpha = (alpha*180) / math.pi\n",
    "                        \n",
    "                        direction = 1\n",
    "                        if y_left_eye < y_right_eye:\n",
    "                            direction = -1\n",
    "                        else:\n",
    "                            direction = 1\n",
    "                            \n",
    "                        alpha = direction*alpha\n",
    "                        \n",
    "                        # print(f\"alpha={alpha}\")\n",
    "\n",
    "                        # finally, align it:\n",
    "                        aligned_face = Image.fromarray(image)\n",
    "                        aligned_face = np.array(aligned_face.rotate(alpha))[:, :, ::-1].copy()\n",
    "\n",
    "                else:\n",
    "                    # print(traceback.format_exc())\n",
    "                    x, y, z = 0.0, 0.0, 0.0\n",
    "                    current_coordinate_set.append([x,y,z]*478)\n",
    "\n",
    "                # print(\"coordinate_count:\", coordinate_count)\n",
    "\n",
    "                # Append detected_keypoints_coordinates:\n",
    "                detected_keypoints_coordinates.append(current_coordinate_set)               \n",
    "\n",
    "                # if verbose:\n",
    "                #    pass  # cv2.imshow('MediaPipe Face Mesh', cv2.flip(image, 1))\n",
    "                # plt.figure()  # figsize=(30,30))\n",
    "                # plt.imshow(image)\n",
    "\n",
    "                # plt.figure()\n",
    "                # plt.imshow(aligned_face)\n",
    "                #####################################################\n",
    "            if aligned_face is not None:\n",
    "                cv2.imshow(\"Image\", aligned_face)\n",
    "            if cv2.waitKey(33) == ord('q'):\n",
    "                break\n",
    "    \n",
    "        except:\n",
    "            print(\"\\n***ERROR***\\n\")\n",
    "            print(traceback.format_exc())\n",
    "            try:\n",
    "                if aligned_face is not None:\n",
    "                    print(aligned_face.shape)\n",
    "                print(\"*******\")\n",
    "            except:\n",
    "                pass\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "            \n",
    "    \n",
    "test_alignment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e04409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "from videosource import WebcamSource\n",
    "\n",
    "from custom.face_geometry import (  # isort:skip\n",
    "    PCF,\n",
    "    get_metric_landmarks,\n",
    "    procrustes_landmark_basis,\n",
    ")\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_face_mesh_connections = mp.solutions.face_mesh_connections\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=3)\n",
    "\n",
    "points_idx = [33, 263, 61, 291, 199]\n",
    "points_idx = points_idx + [key for (key, val) in procrustes_landmark_basis]\n",
    "points_idx = list(set(points_idx))\n",
    "points_idx.sort()\n",
    "\n",
    "# uncomment next line to use all points for PnP algorithm\n",
    "# points_idx = list(range(0,468)); points_idx[0:2] = points_idx[0:2:-1];\n",
    "\n",
    "frame_height, frame_width, channels = (720, 1280, 3)\n",
    "\n",
    "# pseudo camera internals\n",
    "focal_length = frame_width\n",
    "center = (frame_width / 2, frame_height / 2)\n",
    "camera_matrix = np.array(\n",
    "    [[focal_length, 0, center[0]], [0, focal_length, center[1]], [0, 0, 1]],\n",
    "    dtype=\"double\",\n",
    ")\n",
    "\n",
    "dist_coeff = np.zeros((4, 1))\n",
    "\n",
    "\n",
    "def alignment():\n",
    "    source = WebcamSource()\n",
    "\n",
    "    refine_landmarks = True\n",
    "\n",
    "    pcf = PCF(\n",
    "        near=1,\n",
    "        far=10000,\n",
    "        frame_height=frame_height,\n",
    "        frame_width=frame_width,\n",
    "        fy=camera_matrix[1, 1],\n",
    "    )\n",
    "\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=False,\n",
    "        refine_landmarks=refine_landmarks,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5,\n",
    "    ) as face_mesh:\n",
    "\n",
    "        for idx, (frame, frame_rgb) in enumerate(source):\n",
    "            results = face_mesh.process(frame_rgb)\n",
    "            multi_face_landmarks = results.multi_face_landmarks\n",
    "\n",
    "            if multi_face_landmarks:\n",
    "                face_landmarks = multi_face_landmarks[0]\n",
    "                landmarks = np.array(\n",
    "                    [(lm.x, lm.y, lm.z) for lm in face_landmarks.landmark]\n",
    "                )\n",
    "                # print(landmarks.shape)\n",
    "                landmarks = landmarks.T\n",
    "\n",
    "                if refine_landmarks:\n",
    "                    landmarks = landmarks[:, :468]\n",
    "\n",
    "                metric_landmarks, pose_transform_mat = get_metric_landmarks(\n",
    "                    landmarks.copy(), pcf\n",
    "                )\n",
    "\n",
    "                image_points = (\n",
    "                    landmarks[0:2, points_idx].T\n",
    "                    * np.array([frame_width, frame_height])[None, :]\n",
    "                )\n",
    "                model_points = metric_landmarks[0:3, points_idx].T\n",
    "\n",
    "                # see here:\n",
    "                # https://github.com/google/mediapipe/issues/1379#issuecomment-752534379\n",
    "                pose_transform_mat[1:3, :] = -pose_transform_mat[1:3, :]\n",
    "                mp_rotation_vector, _ = cv2.Rodrigues(pose_transform_mat[:3, :3])\n",
    "                mp_translation_vector = pose_transform_mat[:3, 3, None]\n",
    "\n",
    "                if False:\n",
    "                    # sanity check\n",
    "                    # get same result with solvePnP\n",
    "\n",
    "                    success, rotation_vector, translation_vector = cv2.solvePnP(\n",
    "                        model_points,\n",
    "                        image_points,\n",
    "                        camera_matrix,\n",
    "                        dist_coeff,\n",
    "                        flags=cv2.cv2.SOLVEPNP_ITERATIVE,\n",
    "                    )\n",
    "\n",
    "                    np.testing.assert_almost_equal(mp_rotation_vector, rotation_vector)\n",
    "                    np.testing.assert_almost_equal(\n",
    "                        mp_translation_vector, translation_vector\n",
    "                    )\n",
    "\n",
    "                for face_landmarks in multi_face_landmarks:\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        image=frame,\n",
    "                        landmark_list=face_landmarks,\n",
    "                        connections=mp_face_mesh_connections.FACEMESH_TESSELATION,\n",
    "                        landmark_drawing_spec=drawing_spec,\n",
    "                        connection_drawing_spec=drawing_spec,\n",
    "                    )\n",
    "\n",
    "                nose_tip = model_points[0]\n",
    "                nose_tip_extended = 2.5 * model_points[0]\n",
    "                (nose_pointer2D, jacobian) = cv2.projectPoints(\n",
    "                    np.array([nose_tip, nose_tip_extended]),\n",
    "                    mp_rotation_vector,\n",
    "                    mp_translation_vector,\n",
    "                    camera_matrix,\n",
    "                    dist_coeff,\n",
    "                )\n",
    "\n",
    "                nose_tip_2D, nose_tip_2D_extended = nose_pointer2D.squeeze().astype(int)\n",
    "                frame = cv2.line(\n",
    "                    frame, nose_tip_2D, nose_tip_2D_extended, (255, 0, 0), 2\n",
    "                )\n",
    "\n",
    "            source.show(frame)\n",
    "            \n",
    "alignment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54532259",
   "metadata": {},
   "source": [
    "####################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
