{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "896533ad",
   "metadata": {},
   "source": [
    "# Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "536b84ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports:\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from numpy import genfromtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ea94b3",
   "metadata": {},
   "source": [
    "# Dataset & Dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f2adbb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset & Dataloader:\n",
    "\n",
    "# can be None:\n",
    "MEDIA_PIPE_SELECTION = [468,473,282,52,4,0,16,40,90,270,320,199]\n",
    "# cannot be None:\n",
    "OPENFACE_FEATURE_SELECTED = [\"AU01\",\"AU02\",\"AU04\",\"AU05\",\"AU06\",\"AU07\",\"AU09\",\"AU10\",\"AU11\",\"AU12\",\"AU14\",\"AU15\",\"AU17\",\"AU20\",\"AU23\",\"AU24\",\"AU25\",\"AU26\",\"AU28\",\"AU43\",\"anger\",\"disgust\",\"fear\",\"happiness\",\"sadness\",\"surprise\",\"neutral\"]\n",
    "\n",
    "if not (MEDIA_PIPE_SELECTION is None):\n",
    "    MEDIA_PIPE_SELECTION_LEN = len(MEDIA_PIPE_SELECTION)\n",
    "else:\n",
    "    MEDIA_PIPE_SELECTION_LEN = 498  # all\n",
    "    \n",
    "OPENFACE_FEATURE_SELECTED_LEN = len(OPENFACE_FEATURE_SELECTED)\n",
    "\n",
    "\n",
    "\n",
    "def load_coord_data(sample_data_path, frame_cap):\n",
    "    \n",
    "    if sample_data_path.endswith(\".npy\"):\n",
    "        load_sample = np.load(sample_data_path)[:frame_cap]\n",
    "        # print(\"Original shape:\", load_sample.shape)\n",
    "        if MEDIA_PIPE_SELECTION:\n",
    "            load_sample = load_sample[:,MEDIA_PIPE_SELECTION,:]\n",
    "        # print(\"After selection:\", load_sample.shape)\n",
    "    elif sample_data_path.endswith(\".csv\"):\n",
    "        load_sample = pd.read_csv(sample_data_path)[:frame_cap]\n",
    "        if OPENFACE_FEATURE_SELECTED:\n",
    "            load_sample = load_sample[OPENFACE_FEATURE_SELECTED]\n",
    "        load_sample = load_sample.values\n",
    "        return load_sample\n",
    "        # load_sample = load_sample.values()\n",
    "    else:\n",
    "        print(\">>> WARNING: No support for {sample_data_path}. Returning None.\")\n",
    "        load_sample = None\n",
    "    \n",
    "    return load_sample\n",
    "\n",
    "\n",
    "\n",
    "class DeceptionDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, mode, collection_type, data_dir, csv_dir,\n",
    "                 class_to_num={\"truth\": 0, \"lie\": 1}, num_to_class={0: \"truth\", 1: \"lie\"}, transform=None,\n",
    "                 seconds_input_size=3, fps_min=29, keypoints_quantity=478, coordinate_amount=3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.data_df = pd.read_csv(os.path.join(csv_dir, f\"{mode}_DARE.csv\"))\n",
    "        # print(\"frame_sum:\", sum(self.data_df[\"total_frame_count\"].tolist()))\n",
    "        self.transform = transform\n",
    "        if collection_type == \"MediaPipe\":\n",
    "            self.sample_postfix = \"_MP_coord.npy\"\n",
    "        elif collection_type == \"OpenFace\":\n",
    "            self.sample_postfix = \".csv\"\n",
    "        else:\n",
    "            print(\">>> WARNING: No such collection type was used before. Sample postfix append was set to blank.\")\n",
    "            self.sample_postfix = \"\"\n",
    "        self.class_to_num = class_to_num\n",
    "        self.num_to_class = num_to_class\n",
    "        self.frame_cap = fps_min * seconds_input_size\n",
    "        self.input_dim = (self.frame_cap*keypoints_quantity*coordinate_amount)  # torch.Size([4, 87, 478, 3])\n",
    "        print(f\"{mode}'s defined input shape:\", self.input_dim)\n",
    "        self.keypoints_quantity = keypoints_quantity\n",
    "        self.coordinate_amount = coordinate_amount\n",
    "        # 126, 87 (29FPS*3=87), *45*\n",
    "        # 87\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        row_data = self.data_df.iloc[idx]\n",
    "        \n",
    "        # get path to data itself:\n",
    "        base_name = row_data[\"video_name\"].split(\".\")[0]\n",
    "        data_path = os.path.join(self.data_dir,\n",
    "                                 f\"{base_name}{self.sample_postfix}\").replace(\"\\\\\", \"/\")\n",
    "        \n",
    "        # get label:\n",
    "        label = self.class_to_num[row_data[\"label\"]]        \n",
    "        data  = load_coord_data(data_path, self.frame_cap)\n",
    "        \n",
    "        # if FPS is lower, duplicate every frame to increase (generate slow video as workaround):        \n",
    "        if data.shape[0] < self.frame_cap:\n",
    "            repeat_for = int(math.ceil(self.frame_cap / data.shape[0]))\n",
    "            data = np.repeat(data, repeat_for, axis=0)[:self.frame_cap]\n",
    "            # add additional edge case carry out:\n",
    "            if len(data) < self.frame_cap:\n",
    "                extra_needed = self.frame_cap - len(data)\n",
    "                extra_array = np.zeros((extra_needed, data.shape[1], 3), dtype=float)\n",
    "                data = np.concatenate((data, extra_array))\n",
    "        # print(\"Before flatten keypoints X coord shape:\", data.shape)  # (87, 478, 3)  # 87*478*3 = 124758\n",
    "        data = data.reshape(self.frame_cap*self.keypoints_quantity * self.coordinate_amount)\n",
    "        # print(\"After flattening keypoints X coord shape:\", data.shape)\n",
    "                \n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cea829",
   "metadata": {},
   "source": [
    "# LSTM Model Declaration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "80e3c6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # Building your LSTM\n",
    "        # batch_first=True causes input/output tensors to be of shape\n",
    "        # (batch_dim, seq_dim, feature_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True).double()\n",
    "\n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim).double()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        # h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        h0 = torch.zeros(self.layer_dim, self.hidden_dim).requires_grad_().double()\n",
    "\n",
    "        # Initialize cell state\n",
    "        # c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        c0 = torch.zeros(self.layer_dim, self.hidden_dim).requires_grad_().double()\n",
    "\n",
    "        # One time step\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        # Index hidden state of last time step\n",
    "        # out.size() --> 100, 28, 100\n",
    "        # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! \n",
    "        # out = self.fc(out[:, -1, :])  # WAS\n",
    "        # print(\"out.shape:\", out[-1].shape)\n",
    "        # print(\"out.shape:\", out.shape)\n",
    "        out = self.fc(out)  # out[-1])\n",
    "        # out.size() --> 100, 10\n",
    "        m = nn.Sigmoid()\n",
    "        return m(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea619e1",
   "metadata": {},
   "source": [
    "# Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b51d011e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train's defined input shape: 3132\n",
      "val's defined input shape: 3132\n",
      "LSTMModel(\n",
      "  (lstm): LSTM(3132, 100, num_layers=3, batch_first=True)\n",
      "  (fc): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n",
      "14\n",
      "torch.Size([400, 3132])\n",
      "torch.Size([400, 100])\n",
      "torch.Size([400])\n",
      "torch.Size([400])\n",
      "torch.Size([400, 100])\n",
      "torch.Size([400, 100])\n",
      "torch.Size([400])\n",
      "torch.Size([400])\n",
      "torch.Size([400, 100])\n",
      "torch.Size([400, 100])\n",
      "torch.Size([400])\n",
      "torch.Size([400])\n",
      "torch.Size([1, 100])\n",
      "torch.Size([1])\n",
      "Iteration: 50. Loss: 0.6947583280561123. Accuracy: 0.0\n",
      "Iteration: 100. Loss: 0.6946104392013721. Accuracy: 0.0\n",
      "Iteration: 150. Loss: 0.6743771189122384. Accuracy: 0.0\n",
      "Iteration: 200. Loss: 0.684663355590329. Accuracy: 0.0\n",
      "Iteration: 250. Loss: 0.6881782222720956. Accuracy: 0.0\n",
      "Iteration: 300. Loss: 0.6994213170482642. Accuracy: 0.0\n",
      "Iteration: 350. Loss: 0.6958855084979003. Accuracy: 0.0\n",
      "Iteration: 400. Loss: 0.690952617687733. Accuracy: 0.0\n",
      "Iteration: 450. Loss: 0.6987961126153708. Accuracy: 0.0\n",
      "Iteration: 500. Loss: 0.6879614611641014. Accuracy: 0.0\n",
      "Iteration: 550. Loss: 0.6945953382830105. Accuracy: 0.0\n",
      "Iteration: 600. Loss: 0.703945251830607. Accuracy: 0.0\n",
      "Iteration: 650. Loss: 0.6981469366622558. Accuracy: 0.0\n",
      "Iteration: 700. Loss: 0.6838113899848399. Accuracy: 0.0\n",
      "Iteration: 750. Loss: 0.7125389427265338. Accuracy: 0.0\n",
      "Iteration: 800. Loss: 0.6987164185719019. Accuracy: 0.0\n",
      "Iteration: 850. Loss: 0.7076901632924917. Accuracy: 0.0\n",
      "Iteration: 900. Loss: 0.696835762171263. Accuracy: 0.0\n",
      "Iteration: 950. Loss: 0.7047496150651965. Accuracy: 0.0\n",
      "Iteration: 1000. Loss: 0.6941044761399218. Accuracy: 0.0\n",
      "Iteration: 1050. Loss: 0.7068146535860638. Accuracy: 0.0\n",
      "Iteration: 1100. Loss: 0.7040299993193919. Accuracy: 0.0\n",
      "Iteration: 1150. Loss: 0.6796411691848624. Accuracy: 0.0\n",
      "Iteration: 1200. Loss: 0.688374782542957. Accuracy: 0.0\n",
      "Iteration: 1250. Loss: 0.6740827370686664. Accuracy: 0.0\n",
      "Iteration: 1300. Loss: 0.6863315657772708. Accuracy: 0.0\n",
      "Iteration: 1350. Loss: 0.699240401059716. Accuracy: 0.0\n",
      "Iteration: 1400. Loss: 0.6893581887719362. Accuracy: 0.0\n",
      "Iteration: 1450. Loss: 0.7032455383274208. Accuracy: 0.0\n",
      "Iteration: 1500. Loss: 0.7042485150642975. Accuracy: 0.0\n",
      "Iteration: 1550. Loss: 0.689983110463015. Accuracy: 0.0\n",
      "Iteration: 1600. Loss: 0.6875859217099014. Accuracy: 0.0\n",
      "Iteration: 1650. Loss: 0.694969216846644. Accuracy: 0.0\n",
      "Iteration: 1700. Loss: 0.7035286201012354. Accuracy: 0.0\n",
      "Iteration: 1750. Loss: 0.7052318146118637. Accuracy: 0.0\n",
      "Iteration: 1800. Loss: 0.6909358411475723. Accuracy: 0.0\n",
      "Iteration: 1850. Loss: 0.688927276985928. Accuracy: 0.0\n",
      "Iteration: 1900. Loss: 0.698761918399061. Accuracy: 0.0\n",
      "Iteration: 1950. Loss: 0.7029845095481703. Accuracy: 0.0\n",
      "Iteration: 2000. Loss: 0.6868586782424737. Accuracy: 0.0\n",
      "Iteration: 2050. Loss: 0.6841119553211277. Accuracy: 0.0\n",
      "Iteration: 2100. Loss: 0.7110423858441847. Accuracy: 0.0\n",
      "Iteration: 2150. Loss: 0.6836663466215462. Accuracy: 0.0\n",
      "Iteration: 2200. Loss: 0.6991734405121519. Accuracy: 0.0\n",
      "Iteration: 2250. Loss: 0.6883901515354429. Accuracy: 0.0\n",
      "Iteration: 2300. Loss: 0.6752833290830063. Accuracy: 0.0\n",
      "Iteration: 2350. Loss: 0.6935771289412387. Accuracy: 0.0\n",
      "Iteration: 2400. Loss: 0.679231803287424. Accuracy: 0.0\n",
      "Iteration: 2450. Loss: 0.693942645378367. Accuracy: 0.0\n",
      "Iteration: 2500. Loss: 0.6928484086180282. Accuracy: 0.0\n",
      "Iteration: 2550. Loss: 0.7118607730283786. Accuracy: 0.0\n",
      "Iteration: 2600. Loss: 0.6973692950060978. Accuracy: 0.0\n",
      "Iteration: 2650. Loss: 0.7194781129867803. Accuracy: 0.0\n",
      "Iteration: 2700. Loss: 0.6851078780416624. Accuracy: 0.0\n",
      "Iteration: 2750. Loss: 0.6962171645186611. Accuracy: 0.0\n",
      "Iteration: 2800. Loss: 0.6816260630704065. Accuracy: 0.0\n",
      "Iteration: 2850. Loss: 0.6939223022470185. Accuracy: 0.0\n",
      "Iteration: 2900. Loss: 0.6617153835050112. Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "def train(data_type=\"MediaPipe\",\n",
    "          data_root=os.path.join(os.getcwd(), \"mediaPipe_keypoints_data_UPD\").replace(\"\\\\\", \"/\"),\n",
    "          csv_root=os.path.join(\"../data/\").replace(\"\\\\\", \"/\"),\n",
    "          keypoints_quantity=MEDIA_PIPE_SELECTION_LEN,\n",
    "          coordinate_amount=3,\n",
    "          desired_fps=29,\n",
    "          input_in_seconds=3,\n",
    "          batch_size=32,\n",
    "          n_iters = 3000):\n",
    "    \n",
    "    # STEP 1: LOADING DATASET\n",
    "    CAUT_train_dataset = DeceptionDataset(mode=\"train\",\n",
    "                                          collection_type=data_type,  # \"MediaPipe\",\n",
    "                                          data_dir=data_root,  # 'mediaPipe_keypoints_data_UPD'),\n",
    "                                          csv_dir=csv_root,  # audio_data_UPD,\n",
    "                                          class_to_num={\"truth\": 0, \"lie\": 1},\n",
    "                                          num_to_class={0: \"truth\", 1: \"lie\"},\n",
    "                                          transform=None,\n",
    "                                          seconds_input_size=input_in_seconds,\n",
    "                                          fps_min=desired_fps,\n",
    "                                          keypoints_quantity=keypoints_quantity,  # 12,  # 478,\n",
    "                                          coordinate_amount=coordinate_amount)  # 3)\n",
    "    CAUT_val_dataset = DeceptionDataset(mode=\"val\",\n",
    "                                        collection_type=data_type,  # \"MediaPipe\",\n",
    "                                        data_dir=data_root,  # 'mediaPipe_keypoints_data_UPD'),\n",
    "                                        csv_dir=csv_root,  # audio_data_UPD,\n",
    "                                        class_to_num={\"truth\": 0, \"lie\": 1},\n",
    "                                        num_to_class={0: \"truth\", 1: \"lie\"},\n",
    "                                        transform=None,\n",
    "                                        seconds_input_size=input_in_seconds,\n",
    "                                        fps_min=desired_fps,\n",
    "                                        keypoints_quantity=keypoints_quantity,  # 12,  # 478,\n",
    "                                        coordinate_amount=coordinate_amount)  # 3)\n",
    "    \n",
    "    # STEP 2: MAKING DATASET ITERABLE   \n",
    "    train_loader = torch.utils.data.DataLoader(dataset=CAUT_train_dataset, \n",
    "                                               batch_size=batch_size, \n",
    "                                               shuffle=True,\n",
    "                                               drop_last=True)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=CAUT_val_dataset, \n",
    "                                              batch_size=batch_size, \n",
    "                                              shuffle=False,\n",
    "                                              drop_last=True)\n",
    "    \n",
    "    # STEP 3: DECLARE LSTM CLASS (ALREADY DECLARED ABOVE)\n",
    "    \n",
    "    # STEP 4: INSTANTIATE MODEL CLASS\n",
    "    input_dim = CAUT_train_dataset.input_dim\n",
    "    hidden_dim = 100\n",
    "    layer_dim = 3  # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER\n",
    "    output_dim = 1\n",
    "    \n",
    "    model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "    \n",
    "    # JUST PRINTING MODEL & PARAMETERS \n",
    "    print(model)\n",
    "    print(len(list(model.parameters())))\n",
    "    for i in range(len(list(model.parameters()))):\n",
    "        print(list(model.parameters())[i].size())\n",
    "    \n",
    "    \n",
    "    # STEP 5: INSTANTIATE LOSS CLASS\n",
    "    criterion = nn.BCELoss()  # CrossEntropyLoss()\n",
    "    \n",
    "    \n",
    "    # STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "    learning_rate = 0.1\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # STEP 7: TRAIN THE MODEL\n",
    "    iter = 0\n",
    "    num_epochs = n_iters / (len(CAUT_train_dataset) / batch_size)\n",
    "    num_epochs = int(num_epochs)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # for i, (images, labels) in enumerate(train_loader):\n",
    "        for i, (data, labels) in enumerate(train_loader):\n",
    "            data = data.double()\n",
    "            labels = labels.double()\n",
    "            # print(f\"data.shape: {data.shape}\")\n",
    "            # print(f\"labels.shape: {labels.shape}\")\n",
    "            # Load images as Variable\n",
    "            # images = images.view(-1, seq_dim, input_dim).requires_grad_()\n",
    "            # data = data.requires_grad()\n",
    "\n",
    "            # Clear gradients w.r.t. parameters\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass to get output/logits\n",
    "            # outputs.size() --> 100, 10\n",
    "            outputs = model(data).squeeze()\n",
    "            # print(f\"outputs.shape: {outputs.shape}\")\n",
    "            # print(f\"outputs: {outputs}\")\n",
    "\n",
    "            # Calculate Loss: softmax --> cross entropy loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Getting gradients w.r.t. parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # Updating parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            iter += 1\n",
    "\n",
    "            if iter % 50 == 0:\n",
    "                # Calculate Accuracy         \n",
    "                correct = 0\n",
    "                total = 0\n",
    "                # Iterate through test dataset\n",
    "                for data, labels in test_loader:\n",
    "                    data = data.double()\n",
    "                    labels = labels.double()\n",
    "                    # print(f\"data.shape: {data.shape}\")\n",
    "                    # print(f\"labels.shape: {labels.shape}\")\n",
    "                    # Load images to a Torch Variable\n",
    "                    # images = images.view(-1, seq_dim, input_dim).requires_grad_()\n",
    "                    # data = data.requires_grad()\n",
    "\n",
    "                    # Forward pass only to get logits/output\n",
    "                    outputs = model(data).squeeze()\n",
    "                    # print(f\"outputs.shape: {outputs.shape}\")\n",
    "                    # print(f\"outputs: {outputs}\")\n",
    "\n",
    "                    # Get predictions from the maximum value\n",
    "                    # _, predicted = torch.max(outputs.data, 1)\n",
    "                    _, predicted = torch.max(outputs.data, 0)\n",
    "\n",
    "                    # Total number of labels\n",
    "                    total += labels.size(0)\n",
    "\n",
    "                    # Total correct predictions\n",
    "                    correct += (predicted == labels).sum()\n",
    "\n",
    "                accuracy = 100 * correct / total\n",
    "\n",
    "                # Print Loss\n",
    "                print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # for i, (data, labels) in enumerate(train_loader):\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "train()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc96c9c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
